{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "file_extension": ".py"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n\n```\nâ–ˆâ–ˆâ•—   â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•—â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•— â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—\nâ–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•”â•â•â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘â•šâ•â•â–ˆâ–ˆâ•”â•â•â•â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•”â•â•â•â•â•\nâ–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—\nâ•šâ–ˆâ–ˆâ•— â–ˆâ–ˆâ•”â•â–ˆâ–ˆâ•”â•â•â•  â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•”â•â•â–ˆâ–ˆâ•‘â•šâ•â•â•â•â–ˆâ–ˆâ•‘\n â•šâ–ˆâ–ˆâ–ˆâ–ˆâ•”â• â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•—â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘   â–ˆâ–ˆâ•‘  â–ˆâ–ˆâ•‘â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ•‘\n  â•šâ•â•â•â•  â•šâ•â•â•â•â•â•â•â•šâ•â•  â•šâ•â•â•šâ•â•   â•šâ•â•   â•šâ•â•  â•šâ•â•â•šâ•â•â•â•â•â•â•\n```\n\n# ğŸ§  AI Lie Detection System â€” Multimodal Behavioral Analysis\n### *Computer Vision Â· Voice Stress Â· Micro-Expressions Â· NLP Fusion*\n\n---\n\n| Channel | Technology | Weight |\n|---|---|:---:|\n| ğŸ˜¶ Micro-Expressions | OpenCV + Rule Engine | 25% |\n| ğŸ‘ Eye Movement & Blink Rate | OpenCV Landmark Tracking | 20% |\n| ğŸ™ Voice Stress Patterns | Signal Processing (FFT) | 25% |\n| ğŸ§  NLP Sentiment + Deception | Linguistic Pattern Engine | 15% |\n| â± Response Delay / Cognitive Load | Timing Analysis | 10% |\n| ğŸ’“ Physiological Estimation | CV-derived signals | 5% |\n\n> **Truth Score = Weighted Fusion of all 6 channels â†’ 0â€“100% confidence**\n\n</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n## ğŸ“‹ Table of Contents\n\n1. [ğŸ”§ Environment Setup & Imports](#1-environment-setup)\n2. [âš™ï¸ Configuration & Constants](#2-configuration)\n3. [ğŸ›  Utility Functions](#3-utilities)\n4. [ğŸ˜¶ Module 1 â€” Micro-Expression Detector](#4-micro-expression)\n5. [ğŸ‘ Module 2 â€” Eye Movement & Blink Analyzer](#5-eye-movement)\n6. [ğŸ™ Module 3 â€” Voice Stress Analyzer](#6-voice-stress)\n7. [ğŸ§  Module 4 â€” NLP Deception Engine](#7-nlp-engine)\n8. [â± Module 5 â€” Response Delay Tracker](#8-response-delay)\n9. [ğŸ’“ Module 6 â€” Physiological Signal Estimator](#9-physiological)\n10. [ğŸ”€ Score Fusion Engine](#10-score-fusion)\n11. [ğŸ“Š Visualization Dashboard](#11-visualization)\n12. [ğŸ¬ Full Pipeline â€” Run Analysis](#12-full-pipeline)\n13. [ğŸ“„ Report Generator & Export](#13-report)\n14. [ğŸ§ª Demo â€” Synthetic Simulation](#14-demo)\n\n---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n<a id=\"1-environment-setup\"></a>\n## ğŸ”§ Section 1 â€” Environment Setup & Imports\n\nInstall all required packages and import dependencies."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# VERITAS â€” AI Deception Analysis System\n# Install dependencies (run once)\n# ============================================================\n\nimport subprocess, sys\n\ndef install(pkg):\n    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n\nrequired = [\n    \"opencv-python-headless\",\n    \"numpy\",\n    \"pandas\", \n    \"matplotlib\",\n    \"seaborn\",\n    \"scipy\",\n    \"scikit-learn\",\n    \"Pillow\",\n    \"tqdm\",\n    \"ipywidgets\",\n]\n\nprint(\"ğŸ“¦ Installing dependencies...\")\nfor pkg in required:\n    try:\n        install(pkg)\n        print(f\"  âœ… {pkg}\")\n    except Exception as e:\n        print(f\"  âš ï¸  {pkg} â€” {e}\")\n\nprint(\"\\nğŸš€ All packages ready!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# Core Imports\n# ============================================================\n\n# Standard Library\nimport os\nimport sys\nimport time\nimport json\nimport math\nimport random\nimport hashlib\nimport warnings\nimport datetime\nfrom pathlib import Path\nfrom collections import deque, Counter\nfrom dataclasses import dataclass, field, asdict\nfrom typing import List, Dict, Tuple, Optional, Union\nwarnings.filterwarnings(\"ignore\")\n\n# Numeric & Scientific\nimport numpy as np\nimport pandas as pd\nfrom scipy import signal\nfrom scipy.fft import fft, fftfreq\nfrom scipy.stats import zscore, pearsonr\nfrom scipy.signal import butter, filtfilt, find_peaks\n\n# Computer Vision\nimport cv2\n\n# Machine Learning\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.linear_model import LogisticRegression\n\n# Visualization\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.patches import FancyBboxPatch, Circle, Arc\nfrom matplotlib.animation import FuncAnimation\nfrom matplotlib.colors import LinearSegmentedColormap\nimport seaborn as sns\n\n# Image Processing\nfrom PIL import Image, ImageDraw, ImageFont\n\n# Progress & Display\nfrom tqdm.notebook import tqdm\nfrom IPython.display import display, HTML, clear_output, Image as IPImage\nimport ipywidgets as widgets\n\n# â”€â”€â”€ Style Configuration â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nplt.style.use(\"dark_background\")\nmatplotlib.rcParams.update({\n    \"figure.facecolor\":  \"#030812\",\n    \"axes.facecolor\":    \"#080f1e\",\n    \"axes.edgecolor\":    \"#0d2044\",\n    \"axes.labelcolor\":   \"#a8c8e8\",\n    \"text.color\":        \"#a8c8e8\",\n    \"xtick.color\":       \"#3a5a7a\",\n    \"ytick.color\":       \"#3a5a7a\",\n    \"grid.color\":        \"#0d2044\",\n    \"grid.linewidth\":    0.5,\n    \"font.family\":       \"monospace\",\n    \"axes.titlesize\":    13,\n    \"axes.titleweight\":  \"bold\",\n    \"figure.dpi\":        120,\n})\n\nCYAN   = \"#00d4ff\"\nRED    = \"#ff3c6e\"\nGREEN  = \"#39ff14\"\nYELLOW = \"#ffb800\"\nORANGE = \"#ff6b35\"\nDIM    = \"#3a5a7a\"\n\nprint(\"âœ… All imports successful!\")\nprint(f\"   NumPy  {np.__version__}  |  OpenCV {cv2.__version__}  |  Pandas {pd.__version__}  |  Matplotlib {matplotlib.__version__}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n<a id=\"2-configuration\"></a>\n## âš™ï¸ Section 2 â€” Configuration & Constants"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# VERITAS SYSTEM CONFIGURATION\n# ============================================================\n\n@dataclass\nclass VeritasConfig:\n    \"\"\"Central configuration for the VERITAS system.\"\"\"\n\n    # â”€â”€â”€ Channel Weights (must sum to 1.0) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    WEIGHT_MICRO_EXPR:  float = 0.25\n    WEIGHT_EYE_MOVE:    float = 0.20\n    WEIGHT_VOICE_STRESS:float = 0.25\n    WEIGHT_NLP:         float = 0.15\n    WEIGHT_DELAY:       float = 0.10\n    WEIGHT_PHYSIO:      float = 0.05\n\n    # â”€â”€â”€ Verdict Thresholds â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    TRUTHFUL_THRESHOLD:        float = 75.0\n    LIKELY_TRUTHFUL_THRESHOLD: float = 55.0\n    INCONCLUSIVE_THRESHOLD:    float = 40.0\n    LIKELY_DECEPTIVE_THRESHOLD:float = 25.0\n    # Below 25 = DECEPTIVE\n\n    # â”€â”€â”€ Eye Tracking â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    NORMAL_BLINK_MIN: int = 10   # blinks/min\n    NORMAL_BLINK_MAX: int = 20   # blinks/min\n    ELEVATED_BLINK:   int = 30   # blinks/min â€” anxiety threshold\n    GAZE_STABLE_THRESHOLD: float = 0.70  # fraction of time looking forward\n\n    # â”€â”€â”€ Voice Stress â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    SAMPLE_RATE: int = 22050\n    FFT_WINDOW:  int = 2048\n    STRESS_FREQ_LOW:  float = 300.0   # Hz â€” voice fundamental\n    STRESS_FREQ_HIGH: float = 3000.0  # Hz â€” voice harmonic range\n    TREMOR_THRESHOLD: float = 0.15    # normalized tremor index\n\n    # â”€â”€â”€ Response Delay â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    FAST_RESPONSE:    float = 0.4   # seconds â€” spontaneous\n    NORMAL_RESPONSE:  float = 0.7   # seconds â€” normal processing\n    SLOW_RESPONSE:    float = 1.0   # seconds â€” elevated cognitive load\n    HIGH_LATENCY:     float = 1.5   # seconds â€” strong deception marker\n\n    # â”€â”€â”€ Smoothing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    EMA_ALPHA: float = 0.12   # exponential moving average factor\n\n    # â”€â”€â”€ Session â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    TIMELINE_BUFFER: int = 60   # data points in rolling timeline\n    FEED_MAX_EVENTS: int = 200  # max behavioral feed entries\n\n    # â”€â”€â”€ NLP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    HEDGE_RISK_WEIGHT:   float = 4.0\n    PROTEST_RISK_WEIGHT: float = 8.0\n    FILLER_RISK_WEIGHT:  float = 1.5\n    NEGATION_RISK_WEIGHT:float = 3.0\n    DATE_TRUTH_WEIGHT:   float = 3.0\n    NUMBER_TRUTH_WEIGHT: float = 2.0\n\n    def validate(self):\n        total = (self.WEIGHT_MICRO_EXPR + self.WEIGHT_EYE_MOVE +\n                 self.WEIGHT_VOICE_STRESS + self.WEIGHT_NLP +\n                 self.WEIGHT_DELAY + self.WEIGHT_PHYSIO)\n        assert abs(total - 1.0) < 1e-9, f\"Weights must sum to 1.0, got {total:.4f}\"\n        print(f\"  âœ… Config validated â€” weights sum: {total:.2f}\")\n\n\nCFG = VeritasConfig()\nCFG.validate()\nprint(f\"  âœ… VERITAS Config loaded\")\nprint(f\"\\n  Channel Weights:\")\nprint(f\"    Micro-Expression : {CFG.WEIGHT_MICRO_EXPR:.0%}\")\nprint(f\"    Eye Movement     : {CFG.WEIGHT_EYE_MOVE:.0%}\")\nprint(f\"    Voice Stress     : {CFG.WEIGHT_VOICE_STRESS:.0%}\")\nprint(f\"    NLP Analysis     : {CFG.WEIGHT_NLP:.0%}\")\nprint(f\"    Response Delay   : {CFG.WEIGHT_DELAY:.0%}\")\nprint(f\"    Physiological    : {CFG.WEIGHT_PHYSIO:.0%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n<a id=\"3-utilities\"></a>\n## ğŸ›  Section 3 â€” Utility Functions & Data Structures"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# CORE DATA STRUCTURES\n# ============================================================\n\n@dataclass\nclass ChannelScore:\n    \"\"\"Score from a single analysis channel.\"\"\"\n    name: str\n    truth_score: float          # 0â€“100, higher = more truthful\n    deception_risk: float       # 0â€“100, higher = more deceptive\n    confidence: float           # 0â€“1, how confident the engine is\n    flags: List[str] = field(default_factory=list)\n    metadata: Dict  = field(default_factory=dict)\n    timestamp: float = field(default_factory=time.time)\n\n    @property\n    def label(self) -> str:\n        if self.truth_score >= 75: return \"TRUTHFUL\"\n        if self.truth_score >= 55: return \"LIKELY TRUTHFUL\"\n        if self.truth_score >= 40: return \"INCONCLUSIVE\"\n        if self.truth_score >= 25: return \"LIKELY DECEPTIVE\"\n        return \"DECEPTIVE\"\n\n    @property\n    def color(self) -> str:\n        if self.truth_score >= 75: return GREEN\n        if self.truth_score >= 55: return CYAN\n        if self.truth_score >= 40: return YELLOW\n        if self.truth_score >= 25: return ORANGE\n        return RED\n\n\n@dataclass\nclass AnalysisResult:\n    \"\"\"Complete analysis result from the VERITAS engine.\"\"\"\n    session_id: str\n    timestamp: datetime.datetime\n    duration_seconds: float\n    channels: Dict[str, ChannelScore]\n    truth_score: float          # Weighted fusion score\n    deception_risk: float\n    verdict: str\n    flags: List[str]\n    timeline: List[float]       # Rolling truth score history\n    raw_text: str = \"\"\n\n    @property\n    def verdict_color(self) -> str:\n        if self.truth_score >= 75: return GREEN\n        if self.truth_score >= 55: return CYAN\n        if self.truth_score >= 40: return YELLOW\n        if self.truth_score >= 25: return ORANGE\n        return RED\n\n\n# â”€â”€â”€ Utility Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\nclass EMA:\n    \"\"\"Exponential Moving Average smoother.\"\"\"\n    def __init__(self, alpha: float = 0.12):\n        self.alpha = alpha\n        self.value = None\n\n    def update(self, x: float) -> float:\n        if self.value is None:\n            self.value = x\n        else:\n            self.value = self.alpha * x + (1 - self.alpha) * self.value\n        return self.value\n\n    def reset(self):\n        self.value = None\n\n\ndef clamp(x: float, lo: float = 0.0, hi: float = 100.0) -> float:\n    return max(lo, min(hi, x))\n\n\ndef normalize(arr: np.ndarray, lo: float = 0, hi: float = 100) -> np.ndarray:\n    mn, mx = arr.min(), arr.max()\n    if mx == mn: return np.full_like(arr, (lo + hi) / 2, dtype=float)\n    return lo + (arr - mn) / (mx - mn) * (hi - lo)\n\n\ndef verdict_from_score(score: float) -> str:\n    if score >= CFG.TRUTHFUL_THRESHOLD:        return \"âœ… TRUTHFUL\"\n    if score >= CFG.LIKELY_TRUTHFUL_THRESHOLD: return \"ğŸ”µ LIKELY TRUTHFUL\"\n    if score >= CFG.INCONCLUSIVE_THRESHOLD:    return \"ğŸŸ¡ INCONCLUSIVE\"\n    if score >= CFG.LIKELY_DECEPTIVE_THRESHOLD:return \"ğŸŸ  LIKELY DECEPTIVE\"\n    return \"ğŸ”´ DECEPTIVE\"\n\n\ndef color_from_score(score: float) -> str:\n    if score >= 75: return GREEN\n    if score >= 55: return CYAN\n    if score >= 40: return YELLOW\n    if score >= 25: return ORANGE\n    return RED\n\n\ndef generate_session_id() -> str:\n    return hashlib.md5(str(time.time()).encode()).hexdigest()[:8].upper()\n\n\nprint(\"âœ… Data structures & utilities initialized\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n<a id=\"4-micro-expression\"></a>\n## ğŸ˜¶ Module 1 â€” Micro-Expression Detector\n\nDetects involuntary facial muscle movements lasting 40â€“500ms that leak suppressed emotions. Uses OpenCV face detection + optical flow analysis."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# MICRO-EXPRESSION DETECTION ENGINE\n# ============================================================\n\nclass MicroExpressionDetector:\n    \"\"\"\n    Detects micro-expressions using:\n    - OpenCV Haar Cascade face detection\n    - Lucas-Kanade Optical Flow for facial motion vectors\n    - Expression classification via Action Unit (AU) heuristics\n    - Duration gating (40â€“500ms = micro, longer = macro)\n    \"\"\"\n\n    EXPRESSIONS = {\n        \"NEUTRAL\":           {\"risk\": 0,  \"description\": \"Baseline resting state\"},\n        \"MICRO-FEAR\":        {\"risk\": 85, \"description\": \"Involuntary fear flash â€” strong deception signal\"},\n        \"SUPPRESSED SMILE\":  {\"risk\": 60, \"description\": \"Duping delight â€” joy at deceiving\"},\n        \"BROW FURROW\":       {\"risk\": 45, \"description\": \"Cognitive effort / discomfort\"},\n        \"NOSTRIL FLARE\":     {\"risk\": 70, \"description\": \"Fight-or-flight activation\"},\n        \"LIP COMPRESSION\":   {\"risk\": 55, \"description\": \"Suppressed speech / withheld info\"},\n        \"EYE WIDENING\":      {\"risk\": 20, \"description\": \"Surprise â€” genuine emotional response\"},\n        \"CONTEMPT\":          {\"risk\": 75, \"description\": \"One-sided smirk â€” disrespect or deception\"},\n        \"DISGUST FLASH\":     {\"risk\": 50, \"description\": \"Brief disgust micro-expression\"},\n    }\n\n    def __init__(self):\n        self.face_cascade = cv2.CascadeClassifier(\n            cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n        )\n        self.eye_cascade = cv2.CascadeClassifier(\n            cv2.data.haarcascades + \"haarcascade_eye.xml\"\n        )\n        self.prev_gray = None\n        self.prev_points = None\n        self.expression_history = deque(maxlen=50)\n        self.motion_history = deque(maxlen=30)\n        self.ema = EMA(alpha=0.15)\n        self.frame_count = 0\n\n    def detect_face(self, frame: np.ndarray) -> Optional[Tuple]:\n        \"\"\"Returns (x, y, w, h) of largest detected face or None.\"\"\"\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        faces = self.face_cascade.detectMultiScale(\n            gray, scaleFactor=1.1, minNeighbors=5, minSize=(80, 80)\n        )\n        if len(faces) == 0:\n            return None, gray\n        # Return largest face\n        face = max(faces, key=lambda f: f[2] * f[3])\n        return face, gray\n\n    def compute_optical_flow(self, gray: np.ndarray) -> float:\n        \"\"\"Compute mean optical flow magnitude â€” proxy for facial motion.\"\"\"\n        if self.prev_gray is None or self.prev_points is None:\n            self.prev_gray = gray\n            # Select good feature points (corners)\n            self.prev_points = cv2.goodFeaturesToTrack(\n                gray, maxCorners=100, qualityLevel=0.01, minDistance=10\n            )\n            return 0.0\n\n        if self.prev_points is None or len(self.prev_points) < 5:\n            self.prev_gray = gray\n            return 0.0\n\n        new_points, status, _ = cv2.calcOpticalFlowPyrLK(\n            self.prev_gray, gray, self.prev_points, None\n        )\n        if new_points is None:\n            return 0.0\n\n        good_old = self.prev_points[status == 1]\n        good_new = new_points[status == 1]\n\n        if len(good_new) < 3:\n            return 0.0\n\n        displacements = np.linalg.norm(good_new - good_old, axis=1)\n        mean_motion = float(np.mean(displacements))\n\n        self.prev_gray = gray\n        self.prev_points = cv2.goodFeaturesToTrack(\n            gray, maxCorners=100, qualityLevel=0.01, minDistance=10\n        )\n        return mean_motion\n\n    def classify_expression(self, motion: float, face: Optional[Tuple]) -> str:\n        \"\"\"\n        Classify expression from motion magnitude + face region analysis.\n        In production: replace with CNN or FaceMesh AU detector.\n        \"\"\"\n        self.motion_history.append(motion)\n\n        if len(self.motion_history) < 5:\n            return \"NEUTRAL\"\n\n        mean_m = np.mean(self.motion_history)\n        std_m  = np.std(self.motion_history)\n\n        # Spike detection (micro-expression = sudden deviation from baseline)\n        is_spike = (motion > mean_m + 2.0 * std_m) and (motion > 1.5)\n\n        if not is_spike:\n            return \"NEUTRAL\"\n\n        # Classify spike into expression type (heuristic)\n        r = random.random()\n        expressions = list(self.EXPRESSIONS.keys())\n        weights     = [1 if e == \"NEUTRAL\" else 2 for e in expressions]\n        # Higher motion = stronger expression\n        if motion > 4.0:\n            high_risk = [\"MICRO-FEAR\", \"NOSTRIL FLARE\", \"CONTEMPT\"]\n            return random.choice(high_risk)\n        elif motion > 2.5:\n            mid_risk = [\"SUPPRESSED SMILE\", \"LIP COMPRESSION\", \"DISGUST FLASH\", \"BROW FURROW\"]\n            return random.choice(mid_risk)\n        else:\n            return random.choice([\"BROW FURROW\", \"EYE WIDENING\", \"LIP COMPRESSION\"])\n\n    def analyze_frame(self, frame: np.ndarray) -> ChannelScore:\n        \"\"\"Process one video frame and return micro-expression score.\"\"\"\n        self.frame_count += 1\n        face, gray = self.detect_face(frame)\n        motion = self.compute_optical_flow(gray)\n        expression = self.classify_expression(motion, face)\n        self.expression_history.append(expression)\n\n        # Compute truth score from expression risk\n        risk = self.EXPRESSIONS[expression][\"risk\"]\n        recent_risks = [self.EXPRESSIONS[e][\"risk\"]\n                        for e in list(self.expression_history)[-10:]]\n        avg_risk = np.mean(recent_risks) if recent_risks else 0\n        truth = clamp(100 - avg_risk - random.uniform(0, 10))\n        truth = self.ema.update(truth)\n\n        flags = []\n        if avg_risk > 50:\n            flags.append(f\"HIGH-RISK EXPRESSION: {expression}\")\n        if avg_risk > 30:\n            flags.append(\"ELEVATED AROUSAL DETECTED\")\n\n        return ChannelScore(\n            name=\"Micro-Expression\",\n            truth_score=truth,\n            deception_risk=clamp(100 - truth),\n            confidence=0.85 if face is not None else 0.40,\n            flags=flags,\n            metadata={\n                \"expression\": expression,\n                \"motion_magnitude\": round(motion, 3),\n                \"description\": self.EXPRESSIONS[expression][\"description\"],\n                \"face_detected\": face is not None,\n                \"frames_analyzed\": self.frame_count,\n            }\n        )\n\n    def analyze_image(self, img_path: str) -> ChannelScore:\n        \"\"\"Analyze a single image file.\"\"\"\n        frame = cv2.imread(img_path)\n        if frame is None:\n            raise FileNotFoundError(f\"Cannot load image: {img_path}\")\n        return self.analyze_frame(frame)\n\n    def analyze_synthetic(self, n_frames: int = 100,\n                           stress_level: float = 0.5) -> List[ChannelScore]:\n        \"\"\"\n        Generate synthetic analysis results for demo/testing.\n        stress_level: 0.0 (calm/truthful) â†’ 1.0 (stressed/deceptive)\n        \"\"\"\n        scores = []\n        frame_h, frame_w = 480, 640\n        \n        for i in range(n_frames):\n            frame = np.zeros((frame_h, frame_w, 3), dtype=np.uint8)\n            # Add synthetic noise to simulate facial motion\n            noise = np.random.normal(0, 20 + stress_level * 40, frame.shape).astype(np.uint8)\n            frame = cv2.add(frame, noise)\n            # Draw a synthetic \"face\" ellipse\n            cx, cy = frame_w//2 + int(np.sin(i*0.1) * 5), frame_h//2\n            cv2.ellipse(frame, (cx, cy), (80, 100), 0, 0, 360, (200, 180, 160), -1)\n            scores.append(self.analyze_frame(frame))\n        \n        return scores\n\n\n# â”€â”€â”€ Instantiate & Quick Test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nmicro_detector = MicroExpressionDetector()\n\n# Test with a synthetic frame\ntest_frame = np.random.randint(50, 200, (480, 640, 3), dtype=np.uint8)\ntest_score = micro_detector.analyze_frame(test_frame)\n\nprint(\"âœ… MicroExpressionDetector ready\")\nprint(f\"   Expression Catalog  : {len(MicroExpressionDetector.EXPRESSIONS)} types\")\nprint(f\"   Test frame result   : {test_score.expression if hasattr(test_score, 'expression') else test_score.metadata.get('expression', 'N/A')}\")\nprint(f\"   Test truth score    : {test_score.truth_score:.1f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n<a id=\"5-eye-movement\"></a>\n## ğŸ‘ Module 2 â€” Eye Movement & Blink Analyzer\n\nTracks blink rate, gaze direction, and fixation stability â€” strong involuntary signals of arousal and cognitive deception."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# EYE MOVEMENT & BLINK ANALYSIS ENGINE\n# ============================================================\n\nclass EyeMovementAnalyzer:\n    \"\"\"\n    Analyzes:\n    - Blink rate (elevated = arousal/stress)\n    - Eye Aspect Ratio (EAR) for blink detection\n    - Gaze direction (leftward/rightward shift = confabulation)\n    - Gaze stability (unstable = evasive behavior)\n    - Pupil dilation estimation\n    \"\"\"\n\n    EAR_BLINK_THRESHOLD = 0.20   # below this = eye closed\n    EAR_OPEN_MIN        = 0.25\n\n    def __init__(self):\n        self.eye_cascade = cv2.CascadeClassifier(\n            cv2.data.haarcascades + \"haarcascade_eye.xml\"\n        )\n        self.face_cascade = cv2.CascadeClassifier(\n            cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n        )\n        self.blink_count      = 0\n        self.session_start    = time.time()\n        self.gaze_history     = deque(maxlen=100)   # x offset history\n        self.blink_times      = deque(maxlen=50)\n        self.was_blinking     = False\n        self.ear_history      = deque(maxlen=30)\n        self.ema              = EMA(alpha=0.10)\n        self.frame_times      = deque(maxlen=200)\n\n    def compute_ear(self, eye_points: np.ndarray) -> float:\n        \"\"\"Eye Aspect Ratio (EAR) from 6 landmark points.\"\"\"\n        # EAR = (||p2-p6|| + ||p3-p5||) / (2 * ||p1-p4||)\n        if len(eye_points) < 6:\n            return 0.3  # default open\n        A = np.linalg.norm(eye_points[1] - eye_points[5])\n        B = np.linalg.norm(eye_points[2] - eye_points[4])\n        C = np.linalg.norm(eye_points[0] - eye_points[3])\n        return (A + B) / (2.0 * C) if C > 0 else 0.3\n\n    def detect_blink(self, ear: float) -> bool:\n        \"\"\"Detect blink via EAR threshold crossing.\"\"\"\n        is_blink = ear < self.EAR_BLINK_THRESHOLD\n        if is_blink and not self.was_blinking:\n            self.blink_count += 1\n            self.blink_times.append(time.time())\n        self.was_blinking = is_blink\n        return is_blink\n\n    def blink_rate_bpm(self) -> float:\n        \"\"\"Blinks per minute over session so far.\"\"\"\n        elapsed = max(1.0, time.time() - self.session_start)\n        return (self.blink_count / elapsed) * 60\n\n    def gaze_direction(self, eyes: list, frame_w: int) -> str:\n        \"\"\"Classify gaze direction from eye center positions.\"\"\"\n        if not eyes:\n            return \"UNKNOWN\"\n        centers = [(ex + ew//2) for (ex, ey, ew, eh) in eyes]\n        avg_x = np.mean(centers)\n        center = frame_w / 2\n        offset = (avg_x - center) / center  # -1 (far left) to +1 (far right)\n        self.gaze_history.append(offset)\n\n        if offset < -0.15:\n            return \"LEFT\"     # leftward â€” memory retrieval (truth signal)\n        elif offset > 0.15:\n            return \"RIGHT\"    # rightward â€” construction (deception signal)\n        return \"CENTER\"\n\n    def gaze_stability(self) -> float:\n        \"\"\"Fraction of frames where gaze was centered.\"\"\"\n        if not self.gaze_history:\n            return 1.0\n        stable = sum(1 for g in self.gaze_history if abs(g) < 0.15)\n        return stable / len(self.gaze_history)\n\n    def analyze_frame(self, frame: np.ndarray) -> ChannelScore:\n        \"\"\"Analyze one frame for eye movement signals.\"\"\"\n        self.frame_times.append(time.time())\n        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        h, w = frame.shape[:2]\n\n        # Face detection\n        faces = self.face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(60, 60))\n        eyes = []\n        if len(faces) > 0:\n            fx, fy, fw, fh = max(faces, key=lambda f: f[2]*f[3])\n            roi = gray[fy:fy+fh, fx:fx+fw]\n            detected_eyes = self.eye_cascade.detectMultiScale(roi, 1.1, 3)\n            eyes = [(fx+ex, fy+ey, ew, eh) for (ex, ey, ew, eh) in detected_eyes]\n\n        # Simulate EAR (in production: use FaceMesh or dlib 68-point)\n        simulated_ear = 0.30 + random.gauss(0, 0.05)\n        simulated_ear = max(0.05, min(0.45, simulated_ear))\n        self.ear_history.append(simulated_ear)\n        self.detect_blink(simulated_ear)\n\n        gaze = self.gaze_direction(eyes, w)\n        stability = self.gaze_stability()\n        bpm = self.blink_rate_bpm()\n\n        # â”€â”€â”€ Score Computation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        # Blink rate component\n        if bpm < CFG.NORMAL_BLINK_MAX:\n            blink_trust = 100\n        elif bpm < CFG.ELEVATED_BLINK:\n            blink_trust = 100 - (bpm - CFG.NORMAL_BLINK_MAX) * 3\n        else:\n            blink_trust = max(0, 100 - (bpm - CFG.ELEVATED_BLINK) * 5 - 20)\n\n        # Gaze component\n        gaze_trust = stability * 100\n\n        # Rightward gaze penalty (confabulation indicator)\n        if gaze == \"RIGHT\":\n            gaze_trust -= 20\n        elif gaze == \"LEFT\":\n            gaze_trust += 5  # memory retrieval = truthful signal\n\n        truth = clamp(blink_trust * 0.55 + gaze_trust * 0.45)\n        truth = self.ema.update(truth)\n\n        flags = []\n        if bpm > CFG.ELEVATED_BLINK:\n            flags.append(f\"ELEVATED BLINK RATE: {bpm:.0f} bpm (threshold: {CFG.ELEVATED_BLINK})\")\n        if stability < 0.60:\n            flags.append(\"EVASIVE GAZE PATTERN DETECTED\")\n        if gaze == \"RIGHT\":\n            flags.append(\"RIGHTWARD GAZE â€” POTENTIAL CONFABULATION\")\n\n        return ChannelScore(\n            name=\"Eye Movement\",\n            truth_score=truth,\n            deception_risk=clamp(100 - truth),\n            confidence=0.80 if len(eyes) >= 2 else 0.50,\n            flags=flags,\n            metadata={\n                \"blink_count\": self.blink_count,\n                \"blink_rate_bpm\": round(bpm, 1),\n                \"gaze_direction\": gaze,\n                \"gaze_stability\": round(stability, 3),\n                \"eyes_detected\": len(eyes),\n                \"ear\": round(simulated_ear, 3),\n                \"normal_range\": f\"{CFG.NORMAL_BLINK_MIN}â€“{CFG.NORMAL_BLINK_MAX} bpm\",\n            }\n        )\n\n    def analyze_synthetic(self, n_frames: int = 100,\n                           stress_level: float = 0.5) -> List[ChannelScore]:\n        \"\"\"Synthetic demo analysis.\"\"\"\n        scores = []\n        for _ in range(n_frames):\n            frame = np.random.randint(0, 200, (480, 640, 3), dtype=np.uint8)\n            # Stress increases blink frequency simulation\n            if random.random() < stress_level * 0.3:\n                self.blink_count += 1\n            scores.append(self.analyze_frame(frame))\n        return scores\n\n\neye_analyzer = EyeMovementAnalyzer()\ntest_eye = eye_analyzer.analyze_frame(test_frame)\n\nprint(\"âœ… EyeMovementAnalyzer ready\")\nprint(f\"   Blink Rate Normals  : {CFG.NORMAL_BLINK_MIN}â€“{CFG.NORMAL_BLINK_MAX} bpm\")\nprint(f\"   Elevated Threshold  : >{CFG.ELEVATED_BLINK} bpm\")\nprint(f\"   Test result â€” Gaze  : {test_eye.metadata['gaze_direction']}\")\nprint(f\"   Test truth score    : {test_eye.truth_score:.1f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n<a id=\"6-voice-stress\"></a>\n## ğŸ™ Module 3 â€” Voice Stress Analyzer\n\nUses Fast Fourier Transform (FFT) to analyze audio frequency content. Elevated high-frequency energy, pitch variance, and tremor patterns are strong stress markers."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# VOICE STRESS ANALYSIS ENGINE\n# ============================================================\n\nclass VoiceStressAnalyzer:\n    \"\"\"\n    Analyzes audio signals for:\n    - Fundamental frequency (F0) variance â€” pitch instability\n    - High-frequency tremor index (8â€“14 Hz modulation)\n    - Harmonics-to-Noise Ratio (HNR) estimation\n    - Jitter (cycle-to-cycle frequency variation)\n    - Shimmer (cycle-to-cycle amplitude variation)\n    - Speech energy distribution across frequency bands\n    \"\"\"\n\n    STRESS_BANDS = {\n        \"sub_bass\":   (20,   300),\n        \"voice_fund\": (300,  800),\n        \"voice_harm\": (800,  2500),\n        \"sibilance\":  (2500, 5000),\n        \"air\":        (5000, 8000),\n    }\n\n    def __init__(self, sample_rate: int = 22050):\n        self.sr = sample_rate\n        self.f0_history         = deque(maxlen=100)\n        self.energy_history     = deque(maxlen=100)\n        self.stress_history     = deque(maxlen=100)\n        self.ema                = EMA(alpha=0.15)\n        self.segment_count      = 0\n\n    def estimate_f0(self, audio: np.ndarray) -> float:\n        \"\"\"\n        Estimate fundamental frequency (F0) using autocorrelation.\n        Returns frequency in Hz, 0 if no voice detected.\n        \"\"\"\n        # High-pass filter to remove DC\n        audio_hp = audio - np.mean(audio)\n        if np.max(np.abs(audio_hp)) < 0.01:\n            return 0.0  # silence\n\n        # Autocorrelation\n        corr = np.correlate(audio_hp, audio_hp, mode='full')\n        corr = corr[len(corr)//2:]\n\n        # Find first peak after minimum lag (equiv. to max 500Hz)\n        min_lag = int(self.sr / 500)  # 500 Hz max\n        max_lag = int(self.sr / 60)   # 60 Hz min\n\n        if max_lag > len(corr):\n            max_lag = len(corr) - 1\n\n        peak_region = corr[min_lag:max_lag]\n        if len(peak_region) == 0:\n            return 0.0\n\n        peaks, _ = find_peaks(peak_region, height=np.max(peak_region) * 0.4)\n        if len(peaks) == 0:\n            return 0.0\n\n        lag = peaks[0] + min_lag\n        f0 = self.sr / lag\n        return float(f0)\n\n    def compute_fft_bands(self, audio: np.ndarray) -> Dict[str, float]:\n        \"\"\"Compute energy in each frequency band via FFT.\"\"\"\n        n = len(audio)\n        yf = np.abs(fft(audio))[:n//2]\n        xf = fftfreq(n, 1/self.sr)[:n//2]\n\n        band_energies = {}\n        for name, (lo, hi) in self.STRESS_BANDS.items():\n            mask = (xf >= lo) & (xf < hi)\n            band_energies[name] = float(np.mean(yf[mask])) if mask.any() else 0.0\n        return band_energies\n\n    def compute_tremor_index(self, audio: np.ndarray) -> float:\n        \"\"\"\n        Detect 8â€“14 Hz voice tremor (pathological stress marker).\n        High tremor index correlates with anxiety and deception stress.\n        \"\"\"\n        # Envelope extraction via Hilbert transform\n        analytic  = signal.hilbert(audio)\n        envelope  = np.abs(analytic)\n\n        # FFT of envelope â€” look for 8â€“14 Hz modulation\n        env_fft   = np.abs(fft(envelope))[:len(envelope)//2]\n        env_freqs = fftfreq(len(envelope), 1/self.sr)[:len(envelope)//2]\n\n        tremor_mask  = (env_freqs >= 8) & (env_freqs <= 14)\n        total_mask   = env_freqs > 0\n        tremor_energy = env_fft[tremor_mask].sum() if tremor_mask.any() else 0\n        total_energy  = env_fft[total_mask].sum()  if total_mask.any()  else 1\n        return float(tremor_energy / total_energy) if total_energy > 0 else 0.0\n\n    def compute_jitter(self, audio: np.ndarray) -> float:\n        \"\"\"\n        Simplified jitter estimation â€” variance in zero-crossing rate.\n        High jitter = pitch instability = stress marker.\n        \"\"\"\n        zero_crossings = np.where(np.diff(np.sign(audio)))[0]\n        if len(zero_crossings) < 2:\n            return 0.0\n        intervals = np.diff(zero_crossings)\n        if np.mean(intervals) == 0:\n            return 0.0\n        return float(np.std(intervals) / np.mean(intervals))  # coefficient of variation\n\n    def compute_shimmer(self, audio: np.ndarray) -> float:\n        \"\"\"\n        Simplified shimmer â€” amplitude variation between peaks.\n        High shimmer = vocal irregularity = stress.\n        \"\"\"\n        peaks, _ = find_peaks(np.abs(audio), height=np.max(np.abs(audio)) * 0.3)\n        if len(peaks) < 2:\n            return 0.0\n        amps = np.abs(audio)[peaks]\n        return float(np.std(amps) / (np.mean(amps) + 1e-9))\n\n    def analyze_segment(self, audio: np.ndarray) -> ChannelScore:\n        \"\"\"Analyze one audio segment (e.g. 0.5â€“2 seconds).\"\"\"\n        self.segment_count += 1\n\n        # Core measurements\n        f0           = self.estimate_f0(audio)\n        bands        = self.compute_fft_bands(audio)\n        tremor_idx   = self.compute_tremor_index(audio)\n        jitter       = self.compute_jitter(audio)\n        shimmer      = self.compute_shimmer(audio)\n        rms_energy   = float(np.sqrt(np.mean(audio**2)))\n\n        self.f0_history.append(f0)\n        self.stress_history.append(tremor_idx)\n\n        # F0 variance component (higher variance = higher stress)\n        f0_vals = [v for v in self.f0_history if v > 0]\n        f0_var  = np.std(f0_vals) / (np.mean(f0_vals) + 1e-9) if f0_vals else 0\n        f0_trust = clamp(100 - f0_var * 500)\n\n        # Tremor component\n        tremor_trust = clamp(100 - tremor_idx * 400)\n\n        # Jitter component\n        jitter_trust = clamp(100 - jitter * 60)\n\n        # High frequency stress ratio\n        voice_energy = bands[\"voice_fund\"] + bands[\"voice_harm\"]\n        stress_ratio = bands[\"sibilance\"] / (voice_energy + 1e-9)\n        stress_trust = clamp(100 - stress_ratio * 50)\n\n        # Weighted fusion\n        truth = (f0_trust * 0.30 + tremor_trust * 0.35 +\n                 jitter_trust * 0.20 + stress_trust * 0.15)\n        truth = self.ema.update(truth)\n\n        flags = []\n        if tremor_idx > CFG.TREMOR_THRESHOLD:\n            flags.append(f\"VOICE TREMOR DETECTED: {tremor_idx:.3f} (threshold: {CFG.TREMOR_THRESHOLD})\")\n        if f0_var > 0.15:\n            flags.append(f\"PITCH INSTABILITY: variance {f0_var:.3f}\")\n        if jitter > 0.30:\n            flags.append(f\"HIGH JITTER: {jitter:.3f}\")\n\n        return ChannelScore(\n            name=\"Voice Stress\",\n            truth_score=clamp(truth),\n            deception_risk=clamp(100 - truth),\n            confidence=0.90 if rms_energy > 0.01 else 0.30,\n            flags=flags,\n            metadata={\n                \"f0_hz\": round(f0, 2),\n                \"f0_variance\": round(f0_var, 4),\n                \"tremor_index\": round(tremor_idx, 4),\n                \"jitter\": round(jitter, 4),\n                \"shimmer\": round(shimmer, 4),\n                \"rms_energy\": round(rms_energy, 4),\n                \"frequency_bands\": {k: round(v, 4) for k, v in bands.items()},\n                \"segments_analyzed\": self.segment_count,\n            }\n        )\n\n    def generate_synthetic_audio(self, duration: float = 1.0,\n                                 stress_level: float = 0.5,\n                                 f0_hz: float = 150.0) -> np.ndarray:\n        \"\"\"\n        Generate synthetic voice signal with controllable stress level.\n        stress_level: 0.0 = calm, 1.0 = highly stressed/deceptive\n        \"\"\"\n        t = np.linspace(0, duration, int(self.sr * duration))\n        # Base voice fundamental\n        voice = np.sin(2 * np.pi * f0_hz * t)\n        # Add harmonics\n        for h in [2, 3, 4, 5]:\n            voice += (1/h) * np.sin(2 * np.pi * f0_hz * h * t)\n        # Add stress-proportional tremor (8â€“12 Hz AM modulation)\n        tremor_freq = 10.0\n        tremor_mod  = 1 + stress_level * 0.4 * np.sin(2 * np.pi * tremor_freq * t)\n        voice *= tremor_mod\n        # Add F0 jitter (pitch variance)\n        f0_noise = np.cumsum(np.random.randn(len(t)) * stress_level * 0.5)\n        voice += 0.1 * stress_level * np.sin(2 * np.pi * (f0_hz + f0_noise) * t)\n        # Add background noise\n        noise = np.random.randn(len(t)) * 0.05 * (1 + stress_level)\n        voice = voice + noise\n        # Normalize\n        return (voice / (np.max(np.abs(voice)) + 1e-9)).astype(np.float32)\n\n    def analyze_synthetic(self, n_segments: int = 50,\n                           stress_level: float = 0.5) -> List[ChannelScore]:\n        \"\"\"Run synthetic stress analysis for demo.\"\"\"\n        scores = []\n        for _ in range(n_segments):\n            audio = self.generate_synthetic_audio(\n                duration=0.5,\n                stress_level=stress_level + random.gauss(0, 0.1),\n                f0_hz=120 + random.gauss(0, 20)\n            )\n            scores.append(self.analyze_segment(audio))\n        return scores\n\n\nvoice_analyzer = VoiceStressAnalyzer(sample_rate=CFG.SAMPLE_RATE)\n\n# Test with synthetic audio\ntest_audio  = voice_analyzer.generate_synthetic_audio(duration=1.0, stress_level=0.3)\ntest_voice  = voice_analyzer.analyze_segment(test_audio)\n\nprint(\"âœ… VoiceStressAnalyzer ready\")\nprint(f\"   Sample Rate         : {CFG.SAMPLE_RATE} Hz\")\nprint(f\"   Frequency Bands     : {len(VoiceStressAnalyzer.STRESS_BANDS)}\")\nprint(f\"   Test F0             : {test_voice.metadata['f0_hz']} Hz\")\nprint(f\"   Test Tremor Index   : {test_voice.metadata['tremor_index']}\")\nprint(f\"   Test truth score    : {test_voice.truth_score:.1f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n<a id=\"7-nlp-engine\"></a>\n## ğŸ§  Module 4 â€” NLP Deception Engine\n\nAnalyzes speech content for linguistic deception markers. Hedge words, protest phrases, filler words, and lack of specific detail are all documented deception signals."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# NLP LINGUISTIC DECEPTION ENGINE\n# ============================================================\n\nclass NLPDeceptionEngine:\n    \"\"\"\n    Detects deceptive language patterns via:\n    - Hedge word frequency (reduces commitment, signals uncertainty)\n    - Protest phrases (over-assertions of honesty = deception marker)\n    - Filler word density (cognitive load indicator)\n    - Negation pattern analysis\n    - Specificity scoring (dates, numbers, proper nouns = truth signals)\n    - Sentence complexity analysis\n    - First-person pronoun distancing (reduced 'I' = distancing from lie)\n    - Embedded clauses / over-elaboration detection\n    \"\"\"\n\n    # â”€â”€â”€ Lexicons â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    HEDGE_WORDS = {\n        \"maybe\", \"perhaps\", \"possibly\", \"probably\", \"might\", \"could\",\n        \"sort of\", \"kind of\", \"i think\", \"i believe\", \"i suppose\",\n        \"as far as i know\", \"to my knowledge\", \"roughly\", \"approximately\",\n        \"somewhat\", \"virtually\", \"basically\", \"generally\", \"usually\",\n        \"normally\", \"like\", \"you know\", \"i guess\", \"i imagine\", \"seemingly\",\n        \"apparently\", \"it seems\", \"it appears\", \"more or less\", \"in a way\",\n    }\n\n    PROTEST_PHRASES = {\n        \"trust me\", \"i swear\", \"i promise\", \"believe me\", \"honestly\",\n        \"i would never\", \"to tell the truth\", \"the truth is\",\n        \"frankly speaking\", \"let me be clear\", \"as god is my witness\",\n        \"i'm telling you\", \"you have to believe\", \"why would i lie\",\n        \"i'm not lying\", \"i have nothing to hide\", \"i'm being honest\",\n        \"swear to god\", \"on my mother's grave\", \"hand on heart\",\n    }\n\n    FILLER_WORDS = {\n        \"um\", \"uh\", \"er\", \"hmm\", \"ah\", \"like\", \"you know\", \"right\",\n        \"okay\", \"well\", \"so\", \"anyway\", \"basically\", \"literally\",\n        \"actually\", \"obviously\", \"clearly\", \"definitely\",\n    }\n\n    NEGATION_PATTERNS = {\n        \"didn't\", \"don't\", \"won't\", \"can't\", \"couldn't\", \"wouldn't\",\n        \"never\", \"not\", \"no one\", \"nobody\", \"nothing\", \"nowhere\",\n        \"nor\", \"neither\", \"hardly\", \"scarcely\", \"barely\",\n    }\n\n    FIRST_PERSON_SINGULAR = {\"i\", \"me\", \"my\", \"mine\", \"myself\", \"i'm\", \"i've\", \"i'll\"}\n    FIRST_PERSON_PLURAL   = {\"we\", \"us\", \"our\", \"ours\", \"ourselves\"}\n\n    DATE_PATTERNS = {\n        \"monday\", \"tuesday\", \"wednesday\", \"thursday\", \"friday\",\n        \"saturday\", \"sunday\", \"january\", \"february\", \"march\", \"april\",\n        \"may\", \"june\", \"july\", \"august\", \"september\", \"october\",\n        \"november\", \"december\", \"yesterday\", \"last week\", \"this morning\",\n        \"last night\", \"last year\", \"two days ago\",\n    }\n\n    def __init__(self):\n        self.analysis_history = []\n\n    def tokenize(self, text: str) -> List[str]:\n        \"\"\"Simple whitespace + punctuation tokenizer.\"\"\"\n        import re\n        text = text.lower().strip()\n        tokens = re.findall(r\"\\b[\\w']+\\b\", text)\n        return tokens\n\n    def count_pattern(self, text: str, patterns: set) -> Tuple[int, List[str]]:\n        \"\"\"Count occurrences of pattern set in text. Returns (count, matched).\"\"\"\n        text_lower = text.lower()\n        matched = []\n        for pattern in patterns:\n            if pattern in text_lower:\n                count = text_lower.count(pattern)\n                matched.extend([pattern] * count)\n        return len(matched), matched\n\n    def specificity_score(self, text: str, tokens: List[str]) -> float:\n        \"\"\"\n        Score how specific/detailed the statement is.\n        High specificity â†’ truth signal.\n        \"\"\"\n        import re\n        score = 0.0\n        # Numbers (quantity specificity)\n        numbers = re.findall(r'\\b\\d+\\.?\\d*\\b', text)\n        score += len(numbers) * CFG.NUMBER_TRUTH_WEIGHT\n        # Date references\n        date_count, _ = self.count_pattern(text, self.DATE_PATTERNS)\n        score += date_count * CFG.DATE_TRUTH_WEIGHT\n        # Proper nouns (words with initial caps, crude proxy)\n        proper_nouns = [t for t in text.split() if t and t[0].isupper() and len(t) > 2]\n        score += len(proper_nouns) * 1.5\n        # Named locations / places (very crude â€” would use NER in production)\n        location_words = {\"at\", \"in\", \"near\", \"from\", \"to\", \"on\"}\n        location_context = sum(1 for t in tokens if t in location_words)\n        score += location_context * 1.0\n        return score\n\n    def analyze(self, text: str) -> ChannelScore:\n        \"\"\"\n        Full NLP deception analysis of input text.\n        Returns ChannelScore with truth_score, flags, and detailed metadata.\n        \"\"\"\n        tokens    = self.tokenize(text)\n        word_count = len(tokens)\n\n        if word_count == 0:\n            return ChannelScore(\"NLP\", 50, 50, 0.0, [\"NO TEXT PROVIDED\"])\n\n        # â”€â”€â”€ Count Patterns â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        hedge_n,   hedge_matches   = self.count_pattern(text, self.HEDGE_WORDS)\n        protest_n, protest_matches = self.count_pattern(text, self.PROTEST_PHRASES)\n        filler_n,  filler_matches  = self.count_pattern(text, self.FILLER_WORDS)\n        neg_n,     neg_matches     = self.count_pattern(text, self.NEGATION_PATTERNS)\n\n        # â”€â”€â”€ Pronoun Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        i_count  = sum(1 for t in tokens if t in self.FIRST_PERSON_SINGULAR)\n        we_count = sum(1 for t in tokens if t in self.FIRST_PERSON_PLURAL)\n        i_ratio  = i_count / word_count if word_count > 0 else 0\n        # Reduced 'I' usage = psychological distancing = deception signal\n        i_distancing = i_ratio < 0.04 and word_count > 15\n\n        # â”€â”€â”€ Specificity â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        specificity = self.specificity_score(text, tokens)\n\n        # â”€â”€â”€ Sentence Analysis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        sentences  = [s.strip() for s in text.replace(\"!\", \".\").replace(\"?\", \".\").split(\".\") if s.strip()]\n        n_sents    = max(1, len(sentences))\n        avg_sent_len = word_count / n_sents\n\n        # â”€â”€â”€ Deception Score Computation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        deception_score = 50.0  # neutral baseline\n\n        # Penalty: deception indicators\n        deception_score += hedge_n   * CFG.HEDGE_RISK_WEIGHT\n        deception_score += protest_n * CFG.PROTEST_RISK_WEIGHT\n        deception_score += filler_n  * CFG.FILLER_RISK_WEIGHT\n        deception_score += neg_n     * CFG.NEGATION_RISK_WEIGHT\n        if avg_sent_len > 30: deception_score += 8   # over-elaboration\n        if word_count < 8:    deception_score += 12  # evasive brevity\n        if i_distancing:      deception_score += 10  # psychological distancing\n\n        # Reward: truth indicators\n        deception_score -= specificity\n        if i_ratio > 0.08:    deception_score -= 5   # high I-usage = ownership\n        if avg_sent_len < 20: deception_score -= 3   # concise and direct\n\n        deception_score = clamp(deception_score)\n        truth_score     = clamp(100 - deception_score)\n\n        # â”€â”€â”€ Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        flags = []\n        if protest_n > 0:\n            flags.append(f\"PROTEST LANGUAGE: {', '.join(set(protest_matches))}\")\n        if hedge_n > 3:\n            flags.append(f\"EXCESSIVE HEDGING: {hedge_n} instances\")\n        if filler_n > 5:\n            flags.append(f\"HIGH FILLER DENSITY: {filler_n} fillers in {word_count} words\")\n        if word_count < 8:\n            flags.append(\"EVASIVE BREVITY: response too short\")\n        if i_distancing:\n            flags.append(\"PRONOUN DISTANCING: reduced first-person usage\")\n        if avg_sent_len > 30:\n            flags.append(f\"OVER-ELABORATION: avg sentence {avg_sent_len:.0f} words\")\n\n        result = ChannelScore(\n            name=\"NLP Analysis\",\n            truth_score=truth_score,\n            deception_risk=deception_score,\n            confidence=min(0.95, 0.50 + word_count * 0.01),\n            flags=flags,\n            metadata={\n                \"word_count\": word_count,\n                \"sentence_count\": n_sents,\n                \"avg_sentence_length\": round(avg_sent_len, 1),\n                \"hedge_count\": hedge_n,\n                \"hedge_words\": list(set(hedge_matches))[:5],\n                \"protest_count\": protest_n,\n                \"protest_phrases\": list(set(protest_matches))[:3],\n                \"filler_count\": filler_n,\n                \"negation_count\": neg_n,\n                \"specificity_score\": round(specificity, 2),\n                \"i_ratio\": round(i_ratio, 3),\n                \"i_distancing\": i_distancing,\n                \"deception_score_raw\": round(deception_score, 2),\n            }\n        )\n\n        self.analysis_history.append(result)\n        return result\n\n\n# â”€â”€â”€ Test â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nnlp_engine = NLPDeceptionEngine()\n\n# Test Case 1: Deceptive statement\ndeceptive_text = (\n    \"I swear I wasn't there that evening, you have to believe me. \"\n    \"I would never do something like that, honestly. Maybe someone \"\n    \"else did it, I don't know, like I said I wasn't anywhere near.\"\n)\n\n# Test Case 2: Truthful statement\ntruthful_text = (\n    \"I was at the office at 3pm on Tuesday the 14th. I had a meeting \"\n    \"with Sarah and James from accounting. We finished at around 4:30 \"\n    \"and I drove home on Main Street, arriving at approximately 5:15.\"\n)\n\nr1 = nlp_engine.analyze(deceptive_text)\nr2 = nlp_engine.analyze(truthful_text)\n\nprint(\"âœ… NLPDeceptionEngine ready\")\nprint(f\"\\n  Lexicon sizes:\")\nprint(f\"    Hedge words     : {len(NLPDeceptionEngine.HEDGE_WORDS)}\")\nprint(f\"    Protest phrases : {len(NLPDeceptionEngine.PROTEST_PHRASES)}\")\nprint(f\"    Filler words    : {len(NLPDeceptionEngine.FILLER_WORDS)}\")\n\nprint(f\"\\n  Test â€” Deceptive Statement:\")\nprint(f\"    Truth Score     : {r1.truth_score:.1f}%\")\nprint(f\"    Verdict         : {r1.label}\")\nprint(f\"    Protest count   : {r1.metadata['protest_count']}\")\nprint(f\"    Hedge count     : {r1.metadata['hedge_count']}\")\nprint(f\"    Flags           : {len(r1.flags)}\")\n\nprint(f\"\\n  Test â€” Truthful Statement:\")\nprint(f\"    Truth Score     : {r2.truth_score:.1f}%\")\nprint(f\"    Verdict         : {r2.label}\")\nprint(f\"    Specificity     : {r2.metadata['specificity_score']}\")\nprint(f\"    Flags           : {len(r2.flags)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n<a id=\"8-response-delay\"></a>\n## â± Module 5 â€” Response Delay & Cognitive Load Tracker"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# RESPONSE DELAY & COGNITIVE LOAD TRACKER\n# ============================================================\n\nclass ResponseDelayTracker:\n    \"\"\"\n    Measures and interprets the time between question and response onset.\n\n    Cognitive Load Theory: Deceptive responses require real-time story\n    construction, monitoring of the listener, and managing multiple\n    versions of events â€” creating measurable latency spikes.\n\n    Also tracks:\n    - Inter-word pause duration (micro-hesitations)\n    - Speech rate (words per minute)\n    - Pause-to-speech ratio\n    \"\"\"\n\n    def __init__(self):\n        self.response_times  = deque(maxlen=50)\n        self.pause_durations = deque(maxlen=100)\n        self.speech_rates    = deque(maxlen=50)\n        self.question_time   = None\n        self.ema             = EMA(alpha=0.20)\n\n    def mark_question(self):\n        \"\"\"Call this when a question is posed.\"\"\"\n        self.question_time = time.time()\n\n    def mark_response(self, text: str = \"\") -> float:\n        \"\"\"Call this when response begins. Returns latency in seconds.\"\"\"\n        if self.question_time is None:\n            return 0.0\n        latency = time.time() - self.question_time\n        self.response_times.append(latency)\n        self.question_time = None\n\n        if text:\n            words = len(text.split())\n            wpm   = (words / latency) * 60 if latency > 0 else 0\n            self.speech_rates.append(wpm)\n\n        return latency\n\n    def analyze_latency(self, latency_seconds: float,\n                         word_count: int = 0) -> ChannelScore:\n        \"\"\"Score a given response latency.\"\"\"\n        self.response_times.append(latency_seconds)\n        avg_lat = np.mean(self.response_times)\n\n        # â”€â”€â”€ Score from latency â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        if latency_seconds < CFG.FAST_RESPONSE:\n            truth = 95.0   # spontaneous\n            latency_label = \"SPONTANEOUS\"\n        elif latency_seconds < CFG.NORMAL_RESPONSE:\n            truth = 80.0   # normal\n            latency_label = \"NORMAL\"\n        elif latency_seconds < CFG.SLOW_RESPONSE:\n            truth = 60.0   # slightly elevated\n            latency_label = \"ELEVATED\"\n        elif latency_seconds < CFG.HIGH_LATENCY:\n            truth = 35.0   # high cognitive load\n            latency_label = \"HIGH LATENCY\"\n        else:\n            truth = 15.0   # very high â€” strong deception indicator\n            latency_label = \"EXTREME LATENCY\"\n\n        # Bonus: if response is rapid but very short (deflection)\n        if latency_seconds < CFG.FAST_RESPONSE and word_count < 5:\n            truth -= 20\n            latency_label = \"RAPID DEFLECTION\"\n\n        truth = self.ema.update(clamp(truth))\n\n        flags = []\n        if latency_seconds > CFG.HIGH_LATENCY:\n            flags.append(f\"EXTREME RESPONSE DELAY: {latency_seconds:.2f}s\")\n        elif latency_seconds > CFG.SLOW_RESPONSE:\n            flags.append(f\"ELEVATED RESPONSE LATENCY: {latency_seconds:.2f}s\")\n        if latency_label == \"RAPID DEFLECTION\":\n            flags.append(\"RAPID SHORT DEFLECTION DETECTED\")\n\n        return ChannelScore(\n            name=\"Response Delay\",\n            truth_score=clamp(truth),\n            deception_risk=clamp(100 - truth),\n            confidence=0.75,\n            flags=flags,\n            metadata={\n                \"latency_seconds\": round(latency_seconds, 3),\n                \"latency_label\": latency_label,\n                \"average_latency\": round(avg_lat, 3),\n                \"n_responses\": len(self.response_times),\n                \"thresholds\": {\n                    \"fast\": CFG.FAST_RESPONSE,\n                    \"normal\": CFG.NORMAL_RESPONSE,\n                    \"slow\": CFG.SLOW_RESPONSE,\n                    \"high\": CFG.HIGH_LATENCY,\n                }\n            }\n        )\n\n    def analyze_synthetic(self, n: int = 30,\n                           stress_level: float = 0.5) -> List[ChannelScore]:\n        \"\"\"Simulate response delays with varying stress.\"\"\"\n        scores = []\n        for _ in range(n):\n            # Stressed subjects have longer, more variable latencies\n            base_latency = 0.3 + stress_level * 0.8\n            latency = base_latency + random.expovariate(1.0 / (0.3 + stress_level))\n            latency = min(latency, 4.0)\n            scores.append(self.analyze_latency(latency, word_count=random.randint(5, 30)))\n        return scores\n\n\ndelay_tracker = ResponseDelayTracker()\n\n# Test examples\nprint(\"âœ… ResponseDelayTracker ready\")\nprint(\"\\n  Latency Classification:\")\nfor lat in [0.2, 0.5, 0.85, 1.2, 2.0]:\n    s = delay_tracker.analyze_latency(lat)\n    print(f\"    {lat:.1f}s â†’ {s.metadata['latency_label']:20s} â†’ Truth: {s.truth_score:.0f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n<a id=\"9-physiological\"></a>\n## ğŸ’“ Module 6 â€” Physiological Signal Estimator"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# PHYSIOLOGICAL SIGNAL ESTIMATOR (CV-derived)\n# ============================================================\n\nclass PhysiologicalEstimator:\n    \"\"\"\n    Estimates physiological arousal from video frames:\n\n    1. Remote Photoplethysmography (rPPG) â€” estimates heart rate\n       from subtle skin color changes (RGB variation in face ROI)\n    2. Facial Flush Detection â€” increased redness from blood flow\n    3. Perspiration Estimation â€” skin texture changes under stress\n    4. Micro-tremor in face region â€” involuntary movement amplitude\n\n    Note: These are approximations. Clinical systems use dedicated\n    hardware (galvanic skin response, respiration belt, etc.)\n    \"\"\"\n\n    def __init__(self):\n        self.face_cascade  = cv2.CascadeClassifier(\n            cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\"\n        )\n        self.rgb_history   = deque(maxlen=150)   # ~5 seconds at 30fps\n        self.flush_history = deque(maxlen=60)\n        self.ema           = EMA(alpha=0.10)\n        self.frame_count   = 0\n\n    def extract_face_roi(self, frame: np.ndarray) -> Optional[np.ndarray]:\n        \"\"\"Extract forehead ROI for rPPG signal.\"\"\"\n        gray  = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n        faces = self.face_cascade.detectMultiScale(gray, 1.1, 5, minSize=(60, 60))\n        if len(faces) == 0:\n            return None\n        x, y, w, h = max(faces, key=lambda f: f[2]*f[3])\n        # Forehead region (upper 1/4 of face)\n        forehead_y1 = y + int(h * 0.05)\n        forehead_y2 = y + int(h * 0.30)\n        forehead_x1 = x + int(w * 0.15)\n        forehead_x2 = x + int(w * 0.85)\n        return frame[forehead_y1:forehead_y2, forehead_x1:forehead_x2]\n\n    def estimate_rppg(self, roi: np.ndarray) -> Tuple[float, float]:\n        \"\"\"\n        Estimate heart rate from mean RGB of face ROI.\n        Returns (estimated_hr_bpm, confidence).\n        \"\"\"\n        if roi is None or roi.size == 0:\n            return 72.0, 0.1\n        mean_rgb = roi.mean(axis=(0, 1))  # [B, G, R]\n        self.rgb_history.append(mean_rgb)\n\n        if len(self.rgb_history) < 30:\n            return 72.0, 0.2\n\n        rgb_arr = np.array(self.rgb_history)\n        # Green channel has strongest blood volume pulse signal\n        green_channel = rgb_arr[:, 1]\n        # Remove DC component\n        green_ac = green_channel - np.mean(green_channel)\n        # FFT to find dominant frequency\n        n    = len(green_ac)\n        freqs = np.abs(fftfreq(n, d=1/30))[:n//2]   # assume 30fps\n        power = np.abs(fft(green_ac))[:n//2]\n        # HR range: 50â€“150 bpm = 0.83â€“2.5 Hz\n        hr_mask   = (freqs >= 0.83) & (freqs <= 2.5)\n        if not hr_mask.any():\n            return 72.0, 0.2\n        peak_freq = freqs[hr_mask][np.argmax(power[hr_mask])]\n        hr_bpm    = peak_freq * 60\n        # Confidence from spectral peak sharpness\n        confidence = min(0.85, float(np.max(power[hr_mask]) / (np.mean(power) + 1e-9)) * 0.1)\n        return float(hr_bpm), confidence\n\n    def estimate_flush(self, roi: np.ndarray) -> float:\n        \"\"\"Estimate redness (flush) as ratio of R channel to mean.\"\"\"\n        if roi is None or roi.size == 0:\n            return 0.0\n        mean_r = float(roi[:, :, 2].mean())\n        mean_g = float(roi[:, :, 1].mean())\n        mean_b = float(roi[:, :, 0].mean())\n        # Flush = elevated red relative to blue/green\n        flush = (mean_r - (mean_g + mean_b) / 2) / 255.0\n        self.flush_history.append(flush)\n        return clamp(flush * 100, 0, 100)\n\n    def analyze_frame(self, frame: np.ndarray) -> ChannelScore:\n        \"\"\"Extract physiological signals from one frame.\"\"\"\n        self.frame_count += 1\n        roi = self.extract_face_roi(frame)\n\n        hr_bpm, hr_confidence = self.estimate_rppg(roi)\n        flush_pct = self.estimate_flush(roi)\n\n        # Simulate perspiration index (texture changes â€” crude proxy)\n        persp_idx = abs(flush_pct - 30) * 0.5 + random.gauss(10, 5)\n        persp_idx = clamp(persp_idx)\n\n        # â”€â”€â”€ Score Computation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        # HR component: elevated HR = stress\n        if 60 <= hr_bpm <= 90:\n            hr_trust = 90\n        elif 50 <= hr_bpm < 60 or 90 < hr_bpm <= 100:\n            hr_trust = 70\n        elif hr_bpm > 100:\n            hr_trust = max(0, 90 - (hr_bpm - 100) * 2)\n        else:\n            hr_trust = 60\n\n        # Flush component\n        flush_trust = clamp(100 - flush_pct * 1.2)\n\n        # Perspiration component\n        persp_trust = clamp(100 - persp_idx)\n\n        truth = (hr_trust * 0.45 + flush_trust * 0.35 + persp_trust * 0.20)\n        truth = self.ema.update(clamp(truth))\n\n        flags = []\n        if hr_bpm > 100:\n            flags.append(f\"ELEVATED HEART RATE: ~{hr_bpm:.0f} bpm\")\n        if flush_pct > 40:\n            flags.append(f\"FACIAL FLUSH DETECTED: {flush_pct:.0f}%\")\n\n        return ChannelScore(\n            name=\"Physiological\",\n            truth_score=clamp(truth),\n            deception_risk=clamp(100 - truth),\n            confidence=hr_confidence,\n            flags=flags,\n            metadata={\n                \"estimated_hr_bpm\": round(hr_bpm, 1),\n                \"facial_flush_pct\": round(flush_pct, 1),\n                \"perspiration_idx\": round(persp_idx, 1),\n                \"roi_detected\": roi is not None,\n                \"frames_analyzed\": self.frame_count,\n            }\n        )\n\n    def analyze_synthetic(self, n: int = 50, stress_level: float = 0.5) -> List[ChannelScore]:\n        scores = []\n        for i in range(n):\n            frame = np.zeros((480, 640, 3), dtype=np.uint8)\n            # Simulate face region with stress-based redness\n            base_r = int(160 + stress_level * 60 + random.gauss(0, 10))\n            base_g = int(130 + random.gauss(0, 8))\n            base_b = int(120 + random.gauss(0, 8))\n            cv2.ellipse(frame, (320, 240), (100, 130), 0, 0, 360,\n                        (base_b, base_g, min(255, base_r)), -1)\n            scores.append(self.analyze_frame(frame))\n        return scores\n\n\nphysio_estimator = PhysiologicalEstimator()\ntest_physio_frame = np.random.randint(100, 200, (480, 640, 3), dtype=np.uint8)\ntest_physio = physio_estimator.analyze_frame(test_physio_frame)\n\nprint(\"âœ… PhysiologicalEstimator ready\")\nprint(f\"   Estimated HR        : {test_physio.metadata['estimated_hr_bpm']} bpm\")\nprint(f\"   Facial Flush        : {test_physio.metadata['facial_flush_pct']}%\")\nprint(f\"   Test truth score    : {test_physio.truth_score:.1f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n<a id=\"10-score-fusion\"></a>\n## ğŸ”€ Module 7 â€” Score Fusion Engine\n\nFuses all 6 channel scores using weighted average + anomaly detection + temporal smoothing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# SCORE FUSION ENGINE\n# ============================================================\n\nclass ScoreFusionEngine:\n    \"\"\"\n    Fuses scores from all 6 analysis channels into a single\n    Truthfulness Confidence Score using:\n\n    1. Weighted average fusion\n    2. Confidence-adjusted weighting (low-confidence channels penalized)\n    3. Cross-modal consistency check (conflicting channels trigger warning)\n    4. Temporal exponential moving average (EMA) smoothing\n    5. Anomaly detection via Isolation Forest (optional)\n    \"\"\"\n\n    CHANNEL_WEIGHTS = {\n        \"Micro-Expression\": CFG.WEIGHT_MICRO_EXPR,\n        \"Eye Movement\":     CFG.WEIGHT_EYE_MOVE,\n        \"Voice Stress\":     CFG.WEIGHT_VOICE_STRESS,\n        \"NLP Analysis\":     CFG.WEIGHT_NLP,\n        \"Response Delay\":   CFG.WEIGHT_DELAY,\n        \"Physiological\":    CFG.WEIGHT_PHYSIO,\n    }\n\n    def __init__(self):\n        self.ema            = EMA(alpha=CFG.EMA_ALPHA)\n        self.score_history  = deque(maxlen=CFG.TIMELINE_BUFFER)\n        self.channel_history = {k: deque(maxlen=50) for k in self.CHANNEL_WEIGHTS}\n        self.event_log      = []\n        self.session_start  = time.time()\n\n    def fuse(self, channels: Dict[str, ChannelScore]) -> Tuple[float, List[str]]:\n        \"\"\"\n        Fuse channel scores into composite truth score.\n        Returns (truth_score 0â€“100, list_of_flags).\n        \"\"\"\n        total_weight = 0.0\n        weighted_sum = 0.0\n        all_flags    = []\n\n        for name, base_weight in self.CHANNEL_WEIGHTS.items():\n            if name not in channels:\n                continue\n            cs = channels[name]\n            # Confidence-adjusted weight\n            adj_weight = base_weight * max(0.1, cs.confidence)\n            weighted_sum += cs.truth_score * adj_weight\n            total_weight += adj_weight\n            all_flags.extend(cs.flags)\n            self.channel_history[name].append(cs.truth_score)\n\n        raw_score = weighted_sum / total_weight if total_weight > 0 else 50.0\n\n        # Cross-modal consistency check\n        scores_list = [channels[n].truth_score for n in channels]\n        if len(scores_list) >= 2:\n            spread = max(scores_list) - min(scores_list)\n            if spread > 40:\n                all_flags.append(f\"CROSS-MODAL INCONSISTENCY: {spread:.0f}pt spread between channels\")\n\n        # Temporal smoothing\n        smoothed = self.ema.update(raw_score)\n        self.score_history.append(smoothed)\n\n        return clamp(smoothed), all_flags\n\n    def build_result(self, channels: Dict[str, ChannelScore],\n                     raw_text: str = \"\",\n                     duration: float = 0.0) -> AnalysisResult:\n        \"\"\"Build complete AnalysisResult from channel scores.\"\"\"\n        truth_score, flags = self.fuse(channels)\n        verdict = verdict_from_score(truth_score)\n\n        return AnalysisResult(\n            session_id=generate_session_id(),\n            timestamp=datetime.datetime.now(),\n            duration_seconds=duration or (time.time() - self.session_start),\n            channels=channels,\n            truth_score=truth_score,\n            deception_risk=clamp(100 - truth_score),\n            verdict=verdict,\n            flags=list(set(flags)),\n            timeline=list(self.score_history),\n            raw_text=raw_text,\n        )\n\n    def log_event(self, event_type: str, message: str, score: float = None):\n        \"\"\"Add to behavioral event log.\"\"\"\n        self.event_log.append({\n            \"time\": round(time.time() - self.session_start, 2),\n            \"type\": event_type,\n            \"message\": message,\n            \"score\": score,\n        })\n\n\nfusion_engine = ScoreFusionEngine()\nprint(\"âœ… ScoreFusionEngine ready\")\nprint(f\"   Channel Weights:\")\nfor name, w in ScoreFusionEngine.CHANNEL_WEIGHTS.items():\n    bar = \"â–ˆ\" * int(w * 40)\n    print(f\"    {name:20s} {bar:10s} {w:.0%}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n<a id=\"11-visualization\"></a>\n## ğŸ“Š Module 8 â€” Visualization Dashboard\n\nFull matplotlib dashboard displaying all analysis channels, truth score history, and verdict."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# VISUALIZATION DASHBOARD\n# ============================================================\n\nclass VeritasDashboard:\n    \"\"\"\n    Renders a comprehensive multi-panel analysis dashboard using matplotlib.\n    Panels:\n      [1] Truth Score Gauge (ring chart)\n      [2] Channel Score Bar Chart\n      [3] Timeline History\n      [4] Frequency Band Spectrum (voice)\n      [5] NLP Linguistic Breakdown\n      [6] Eye Movement Heatmap\n      [7] Deception Flag Summary\n      [8] Session Stats Table\n    \"\"\"\n\n    def __init__(self):\n        self.CMAP = LinearSegmentedColormap.from_list(\n            \"veritas\", [\"#ff3c6e\", \"#ffb800\", \"#00d4ff\", \"#39ff14\"], N=256\n        )\n\n    def score_color(self, score: float) -> str:\n        return color_from_score(score)\n\n    def render(self, result: AnalysisResult, figsize=(20, 14)) -> plt.Figure:\n        \"\"\"Render the complete dashboard figure.\"\"\"\n        fig = plt.figure(figsize=figsize, facecolor=\"#030812\")\n        fig.suptitle(\n            \"VERITAS â€” DECEPTION ANALYSIS REPORT\",\n            fontsize=16, fontweight=\"bold\",\n            color=CYAN, y=0.98, fontfamily=\"monospace\", letterspacing=3\n        )\n\n        gs = gridspec.GridSpec(\n            3, 4, figure=fig,\n            hspace=0.45, wspace=0.35,\n            top=0.93, bottom=0.06, left=0.06, right=0.97\n        )\n\n        # â”€ Panel 1: Truth Gauge â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        ax1 = fig.add_subplot(gs[0, 0], polar=True)\n        self._draw_gauge(ax1, result.truth_score, result.verdict)\n\n        # â”€ Panel 2: Channel Scores â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        ax2 = fig.add_subplot(gs[0, 1:3])\n        self._draw_channel_bars(ax2, result.channels)\n\n        # â”€ Panel 3: Session Info â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        ax3 = fig.add_subplot(gs[0, 3])\n        self._draw_session_info(ax3, result)\n\n        # â”€ Panel 4: Timeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        ax4 = fig.add_subplot(gs[1, :2])\n        self._draw_timeline(ax4, result.timeline)\n\n        # â”€ Panel 5: Voice Spectrum â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        ax5 = fig.add_subplot(gs[1, 2])\n        voice = result.channels.get(\"Voice Stress\")\n        if voice:\n            self._draw_voice_spectrum(ax5, voice)\n\n        # â”€ Panel 6: NLP Breakdown â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        ax6 = fig.add_subplot(gs[1, 3])\n        nlp = result.channels.get(\"NLP Analysis\")\n        if nlp:\n            self._draw_nlp_breakdown(ax6, nlp)\n\n        # â”€ Panel 7: Deception Flags â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        ax7 = fig.add_subplot(gs[2, :2])\n        self._draw_flags(ax7, result.flags)\n\n        # â”€ Panel 8: Confidence Heatmap â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        ax8 = fig.add_subplot(gs[2, 2:])\n        self._draw_confidence_heatmap(ax8, result.channels)\n\n        return fig\n\n    def _draw_gauge(self, ax, score: float, verdict: str):\n        \"\"\"Semi-circular truth gauge.\"\"\"\n        ax.set_facecolor(\"#080f1e\")\n        score_c = clamp(score)\n        # Background arc\n        theta_bg = np.linspace(np.pi, 0, 200)\n        ax.plot(theta_bg, [1]*200, color=DIM, linewidth=18, alpha=0.3)\n        # Score arc\n        angle_end = np.pi - (score_c / 100) * np.pi\n        theta_sc  = np.linspace(np.pi, angle_end, 200)\n        sc_color  = self.score_color(score_c)\n        ax.plot(theta_sc, [1]*200, color=sc_color, linewidth=18, alpha=0.9, solid_capstyle=\"round\")\n        # Score text\n        ax.text(np.pi/2, 0.2, f\"{score_c:.0f}%\",\n                ha=\"center\", va=\"center\", fontsize=28, fontweight=\"bold\",\n                color=sc_color, fontfamily=\"monospace\")\n        ax.text(np.pi/2, -0.15, verdict.replace(\"âœ… \",\"\").replace(\"ğŸ”µ \",\"\").replace(\"ğŸŸ¡ \",\"\").replace(\"ğŸŸ  \",\"\").replace(\"ğŸ”´ \",\"\"),\n                ha=\"center\", va=\"center\", fontsize=9,\n                color=sc_color, fontfamily=\"monospace\")\n        ax.text(np.pi/2, -0.42, \"TRUTH SCORE\",\n                ha=\"center\", va=\"center\", fontsize=7,\n                color=DIM, fontfamily=\"monospace\")\n        ax.set_ylim(-0.6, 1.3)\n        ax.axis(\"off\")\n        ax.set_title(\"CONFIDENCE GAUGE\", color=CYAN, fontsize=9,\n                     pad=6, fontfamily=\"monospace\")\n\n    def _draw_channel_bars(self, ax, channels: Dict[str, ChannelScore]):\n        \"\"\"Horizontal bar chart of all channel truth scores.\"\"\"\n        ax.set_facecolor(\"#080f1e\")\n        names  = list(channels.keys())\n        scores = [channels[n].truth_score for n in names]\n        confs  = [channels[n].confidence  for n in names]\n        colors = [self.score_color(s) for s in scores]\n\n        y_pos = range(len(names))\n        bars  = ax.barh(y_pos, scores, color=colors, alpha=0.75, height=0.55,\n                        edgecolor=\"none\")\n        # Confidence indicator\n        for i, (s, c) in enumerate(zip(scores, confs)):\n            ax.barh(i, s * c, color=\"white\", alpha=0.12, height=0.55)\n            ax.text(s + 1, i, f\"{s:.0f}%\", va=\"center\", fontsize=9,\n                    color=colors[i], fontfamily=\"monospace\", fontweight=\"bold\")\n            ax.text(-1, i, f\"{c:.0%}\", va=\"center\", ha=\"right\", fontsize=7,\n                    color=DIM, fontfamily=\"monospace\")\n\n        ax.set_yticks(list(y_pos))\n        ax.set_yticklabels(names, fontsize=9, fontfamily=\"monospace\")\n        ax.set_xlim(-5, 115)\n        ax.set_xlabel(\"Truth Score (%)\", color=DIM, fontsize=8)\n        ax.tick_params(colors=DIM, labelsize=8)\n        ax.axvline(50, color=YELLOW, linewidth=0.7, linestyle=\"--\", alpha=0.5)\n        ax.axvline(75, color=GREEN,  linewidth=0.7, linestyle=\"--\", alpha=0.5)\n        ax.grid(axis=\"x\", alpha=0.2)\n        ax.set_title(\"CHANNEL TRUTH SCORES  (opacity = confidence)\", color=CYAN,\n                     fontsize=9, fontfamily=\"monospace\")\n\n    def _draw_session_info(self, ax, result: AnalysisResult):\n        \"\"\"Session metadata text panel.\"\"\"\n        ax.set_facecolor(\"#080f1e\")\n        ax.axis(\"off\")\n        info_lines = [\n            (\"SESSION ID\",  result.session_id),\n            (\"TIMESTAMP\",   result.timestamp.strftime(\"%H:%M:%S\")),\n            (\"DURATION\",    f\"{result.duration_seconds:.1f}s\"),\n            (\"VERDICT\",     result.verdict.split(\" \", 1)[-1]),\n            (\"TRUTH SCORE\", f\"{result.truth_score:.1f}%\"),\n            (\"RISK LEVEL\",  f\"{result.deception_risk:.1f}%\"),\n            (\"FLAGS\",       str(len(result.flags))),\n            (\"CHANNELS\",    str(len(result.channels))),\n        ]\n        for i, (label, value) in enumerate(info_lines):\n            y = 0.95 - i * 0.115\n            ax.text(0.02, y, label + \":\", transform=ax.transAxes,\n                    fontsize=7.5, color=DIM, fontfamily=\"monospace\")\n            ax.text(0.98, y, value, transform=ax.transAxes,\n                    fontsize=8, color=CYAN, fontfamily=\"monospace\",\n                    ha=\"right\", fontweight=\"bold\")\n            ax.axhline(y - 0.045, xmin=0, xmax=1, color=\"#0d2044\",\n                       linewidth=0.5, transform=ax.transAxes)\n        ax.set_title(\"SESSION METADATA\", color=CYAN, fontsize=9, fontfamily=\"monospace\")\n\n    def _draw_timeline(self, ax, timeline: List[float]):\n        \"\"\"Truth score over time with shaded regions.\"\"\"\n        ax.set_facecolor(\"#080f1e\")\n        if not timeline:\n            ax.text(0.5, 0.5, \"NO DATA\", ha=\"center\", va=\"center\",\n                    color=DIM, transform=ax.transAxes, fontfamily=\"monospace\")\n            ax.set_title(\"TRUTH SCORE TIMELINE\", color=CYAN, fontsize=9, fontfamily=\"monospace\")\n            return\n\n        x = np.arange(len(timeline))\n        y = np.array(timeline)\n        # Gradient fill\n        ax.fill_between(x, 0, y, where=y>=75, alpha=0.15, color=GREEN,    interpolate=True)\n        ax.fill_between(x, 0, y, where=(y>=55)&(y<75), alpha=0.15, color=CYAN,   interpolate=True)\n        ax.fill_between(x, 0, y, where=(y>=40)&(y<55), alpha=0.15, color=YELLOW, interpolate=True)\n        ax.fill_between(x, 0, y, where=y<40,  alpha=0.15, color=RED,     interpolate=True)\n        # Main line\n        ax.plot(x, y, color=CYAN, linewidth=1.5, alpha=0.9)\n        # Threshold lines\n        for thresh, color, label in [(75, GREEN, \"TRUTHFUL\"), (55, CYAN, \"LIKELY T.\"),\n                                      (40, YELLOW, \"INCON.\"), (25, RED, \"DECEPTIVE\")]:\n            ax.axhline(thresh, color=color, linewidth=0.6, linestyle=\"--\", alpha=0.5)\n            ax.text(len(x) + 0.5, thresh, label, fontsize=6, color=color,\n                    fontfamily=\"monospace\", va=\"center\")\n        ax.set_xlim(0, max(len(x), 1))\n        ax.set_ylim(0, 100)\n        ax.set_xlabel(\"Time Steps\", color=DIM, fontsize=8)\n        ax.set_ylabel(\"Truth Score %\", color=DIM, fontsize=8)\n        ax.tick_params(colors=DIM, labelsize=7)\n        ax.grid(alpha=0.15)\n        ax.set_title(\"TRUTH SCORE TIMELINE\", color=CYAN, fontsize=9, fontfamily=\"monospace\")\n\n    def _draw_voice_spectrum(self, ax, voice_score: ChannelScore):\n        \"\"\"Voice frequency band energy visualization.\"\"\"\n        ax.set_facecolor(\"#080f1e\")\n        bands = voice_score.metadata.get(\"frequency_bands\", {})\n        if not bands:\n            ax.text(0.5, 0.5, \"NO AUDIO\", ha=\"center\", va=\"center\",\n                    color=DIM, transform=ax.transAxes, fontfamily=\"monospace\")\n            ax.set_title(\"VOICE SPECTRUM\", color=CYAN, fontsize=9, fontfamily=\"monospace\")\n            return\n        names  = list(bands.keys())\n        values = list(bands.values())\n        # Color by stress level (higher sibilance/air = stress)\n        colors = [GREEN if i < 2 else YELLOW if i < 3 else RED for i in range(len(names))]\n        ax.bar(names, values, color=colors, alpha=0.7, edgecolor=\"none\")\n        ax.set_xticklabels([n.replace(\"_\", \"\\n\") for n in names], fontsize=7,\n                            fontfamily=\"monospace\", color=DIM)\n        ax.tick_params(colors=DIM, labelsize=7)\n        ax.set_ylabel(\"Energy\", color=DIM, fontsize=7)\n        ax.grid(axis=\"y\", alpha=0.2)\n        ax.set_title(\"VOICE FREQ BANDS\", color=CYAN, fontsize=9, fontfamily=\"monospace\")\n\n    def _draw_nlp_breakdown(self, ax, nlp_score: ChannelScore):\n        \"\"\"NLP signal counts as color-coded bar chart.\"\"\"\n        ax.set_facecolor(\"#080f1e\")\n        m = nlp_score.metadata\n        labels = [\"Hedges\", \"Protests\", \"Fillers\", \"Negations\", \"Specificity\"]\n        values = [\n            m.get(\"hedge_count\", 0),\n            m.get(\"protest_count\", 0),\n            m.get(\"filler_count\", 0),\n            m.get(\"negation_count\", 0),\n            min(10, m.get(\"specificity_score\", 0)),\n        ]\n        colors = [RED, RED, YELLOW, ORANGE, GREEN]\n        bars   = ax.bar(labels, values, color=colors, alpha=0.7, edgecolor=\"none\")\n        ax.set_xticklabels(labels, fontsize=7, fontfamily=\"monospace\",\n                            rotation=20, ha=\"right\", color=DIM)\n        ax.tick_params(colors=DIM, labelsize=7)\n        ax.set_ylabel(\"Count / Score\", color=DIM, fontsize=7)\n        ax.grid(axis=\"y\", alpha=0.2)\n        ax.set_title(\"NLP SIGNAL BREAKDOWN\", color=CYAN, fontsize=9, fontfamily=\"monospace\")\n\n    def _draw_flags(self, ax, flags: List[str]):\n        \"\"\"Deception flags as scrollable text.\"\"\"\n        ax.set_facecolor(\"#080f1e\")\n        ax.axis(\"off\")\n        ax.set_title(\"DECEPTION FLAGS\", color=CYAN, fontsize=9, fontfamily=\"monospace\")\n        if not flags:\n            ax.text(0.5, 0.5, \"âœ…  NO FLAGS DETECTED â€” CLEAR\",\n                    ha=\"center\", va=\"center\", fontsize=10,\n                    color=GREEN, fontfamily=\"monospace\", transform=ax.transAxes)\n            return\n        for i, flag in enumerate(flags[:8]):\n            y = 0.88 - i * 0.11\n            ax.text(0.01, y, f\"âš   {flag}\", transform=ax.transAxes,\n                    fontsize=7.5, color=RED, fontfamily=\"monospace\",\n                    bbox=dict(boxstyle=\"round,pad=0.2\", facecolor=\"#1a0a0f\",\n                              edgecolor=\"#ff3c6e\", alpha=0.6))\n\n    def _draw_confidence_heatmap(self, ax, channels: Dict[str, ChannelScore]):\n        \"\"\"Heatmap of truth scores for each channel over simulated time.\"\"\"\n        ax.set_facecolor(\"#080f1e\")\n        ax.set_title(\"CHANNEL CONFIDENCE MAP\", color=CYAN, fontsize=9, fontfamily=\"monospace\")\n        names  = list(channels.keys())\n        scores = [[channels[n].truth_score] for n in names]\n        scores_arr = np.array(scores)\n        im = ax.imshow(scores_arr, cmap=self.CMAP, vmin=0, vmax=100,\n                       aspect=\"auto\", interpolation=\"nearest\")\n        ax.set_yticks(range(len(names)))\n        ax.set_yticklabels(names, fontsize=7, fontfamily=\"monospace\")\n        ax.set_xticks([])\n        ax.tick_params(colors=DIM, labelsize=7)\n        for i, n in enumerate(names):\n            ax.text(0, i, f\"{channels[n].truth_score:.0f}%\",\n                    ha=\"center\", va=\"center\", fontsize=9,\n                    color=\"white\", fontweight=\"bold\", fontfamily=\"monospace\")\n        cbar = plt.colorbar(im, ax=ax, shrink=0.8)\n        cbar.ax.tick_params(colors=DIM, labelsize=7)\n        cbar.set_label(\"Truth Score %\", color=DIM, fontsize=7)\n\n\ndashboard = VeritasDashboard()\nprint(\"âœ… VeritasDashboard ready\")\nprint(\"   Panels: Gauge | Channels | Session | Timeline | Voice | NLP | Flags | Heatmap\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n<a id=\"12-full-pipeline\"></a>\n## ğŸ¬ Module 9 â€” Full Analysis Pipeline\n\nThe `VeritasPipeline` orchestrates all modules together."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# VERITAS FULL ANALYSIS PIPELINE\n# ============================================================\n\nclass VeritasPipeline:\n    \"\"\"\n    Orchestrates all 6 analysis modules into a unified pipeline.\n\n    Supports 3 operation modes:\n    1. Live camera + microphone (requires device access)\n    2. File-based analysis (video file + audio file)\n    3. Synthetic simulation (demo mode â€” no hardware required)\n    \"\"\"\n\n    def __init__(self):\n        self.micro_detector  = MicroExpressionDetector()\n        self.eye_analyzer    = EyeMovementAnalyzer()\n        self.voice_analyzer  = VoiceStressAnalyzer(CFG.SAMPLE_RATE)\n        self.nlp_engine      = NLPDeceptionEngine()\n        self.delay_tracker   = ResponseDelayTracker()\n        self.physio_estimator= PhysiologicalEstimator()\n        self.fusion          = ScoreFusionEngine()\n        self.dashboard       = VeritasDashboard()\n        print(\"âœ… VERITAS Pipeline initialized â€” all modules online\")\n        print(\"   Micro-Expression  |  Eye Movement  |  Voice Stress\")\n        print(\"   NLP Engine        |  Delay Tracker |  Physiological\")\n\n    def analyze_text(self, text: str,\n                     response_latency: float = 0.5) -> AnalysisResult:\n        \"\"\"\n        NLP-only analysis for text input.\n        For quick analysis of a written statement.\n        \"\"\"\n        nlp_score   = self.nlp_engine.analyze(text)\n        delay_score = self.delay_tracker.analyze_latency(response_latency,\n                                                          word_count=len(text.split()))\n        channels = {\n            \"NLP Analysis\":   nlp_score,\n            \"Response Delay\": delay_score,\n        }\n        return self.fusion.build_result(channels, raw_text=text)\n\n    def analyze_synthetic(self,\n                           n_frames: int = 80,\n                           n_audio_segments: int = 40,\n                           text: str = \"\",\n                           stress_level: float = 0.5,\n                           response_latency: float = 0.6,\n                           show_progress: bool = True) -> AnalysisResult:\n        \"\"\"\n        Full synthetic analysis â€” runs all 6 channels with\n        controllable stress level. Perfect for demo and testing.\n\n        Args:\n            n_frames           : Number of video frames to simulate\n            n_audio_segments   : Number of audio segments to analyze\n            text               : Statement text for NLP analysis\n            stress_level       : 0.0 = calm/truthful, 1.0 = highly deceptive\n            response_latency   : Simulated response time in seconds\n            show_progress      : Show tqdm progress bars\n        \"\"\"\n        start = time.time()\n        print(f\"\\nğŸ”¬ VERITAS SYNTHETIC ANALYSIS\")\n        print(f\"   Stress Level      : {stress_level:.0%} ({'HIGH DECEPTION' if stress_level > 0.6 else 'MODERATE' if stress_level > 0.3 else 'LOW / TRUTHFUL'})\")\n        print(f\"   Video Frames      : {n_frames}\")\n        print(f\"   Audio Segments    : {n_audio_segments}\")\n        print(f\"   Response Latency  : {response_latency:.2f}s\")\n        print()\n\n        # â”€â”€â”€ Channel 1: Micro-Expressions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        print(\"  [1/6] Analyzing micro-expressions...\")\n        micro_scores  = self.micro_detector.analyze_synthetic(n_frames, stress_level)\n        avg_micro     = np.mean([s.truth_score for s in micro_scores])\n        final_micro   = micro_scores[-1]\n        final_micro.truth_score     = avg_micro\n        final_micro.deception_risk  = 100 - avg_micro\n\n        # â”€â”€â”€ Channel 2: Eye Movement â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        print(\"  [2/6] Analyzing eye movement & blink rate...\")\n        eye_scores    = self.eye_analyzer.analyze_synthetic(n_frames, stress_level)\n        avg_eye       = np.mean([s.truth_score for s in eye_scores])\n        final_eye     = eye_scores[-1]\n        final_eye.truth_score    = avg_eye\n        final_eye.deception_risk = 100 - avg_eye\n\n        # â”€â”€â”€ Channel 3: Voice Stress â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        print(\"  [3/6] Analyzing voice stress patterns...\")\n        voice_scores  = self.voice_analyzer.analyze_synthetic(n_audio_segments, stress_level)\n        avg_voice     = np.mean([s.truth_score for s in voice_scores])\n        final_voice   = voice_scores[-1]\n        final_voice.truth_score    = avg_voice\n        final_voice.deception_risk = 100 - avg_voice\n        # Ensure frequency bands are populated\n        if not final_voice.metadata.get(\"frequency_bands\"):\n            test_audio = self.voice_analyzer.generate_synthetic_audio(1.0, stress_level)\n            bands = self.voice_analyzer.compute_fft_bands(test_audio)\n            final_voice.metadata[\"frequency_bands\"] = {k: round(v, 4) for k, v in bands.items()}\n\n        # â”€â”€â”€ Channel 4: NLP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        print(\"  [4/6] Running NLP deception analysis...\")\n        if not text:\n            # Generate a synthetic statement based on stress level\n            if stress_level > 0.6:\n                text = (\"Trust me, I swear I wasn't anywhere near there. \"\n                        \"You have to believe me, I would never do something like that. \"\n                        \"I don't know what you're talking about honestly, \"\n                        \"maybe someone else did it, I'm not sure.\")\n            elif stress_level > 0.3:\n                text = (\"I think I was at home, or maybe the office? \"\n                        \"I'm not entirely sure, it's kind of hard to remember. \"\n                        \"Roughly around that time, yes I believe so.\")\n            else:\n                text = (\"I was at the office at 3pm on Tuesday the 14th. \"\n                        \"I had a meeting with Sarah from accounting that lasted 90 minutes. \"\n                        \"I drove home on Main Street and arrived at my house at 5:20pm.\")\n        final_nlp = self.nlp_engine.analyze(text)\n\n        # â”€â”€â”€ Channel 5: Response Delay â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        print(\"  [5/6] Measuring response delay & cognitive load...\")\n        delay_scores  = self.delay_tracker.analyze_synthetic(20, stress_level)\n        avg_delay     = np.mean([s.truth_score for s in delay_scores])\n        final_delay   = self.delay_tracker.analyze_latency(\n            response_latency + stress_level * 0.5,\n            word_count=len(text.split())\n        )\n        final_delay.truth_score    = (avg_delay + final_delay.truth_score) / 2\n        final_delay.deception_risk = 100 - final_delay.truth_score\n\n        # â”€â”€â”€ Channel 6: Physiological â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        print(\"  [6/6] Estimating physiological signals...\")\n        physio_scores = self.physio_estimator.analyze_synthetic(30, stress_level)\n        avg_physio    = np.mean([s.truth_score for s in physio_scores])\n        final_physio  = physio_scores[-1]\n        final_physio.truth_score    = avg_physio\n        final_physio.deception_risk = 100 - avg_physio\n\n        # â”€â”€â”€ Build timeline â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        print(\"\\n  âš¡ Fusing channels...\")\n        all_channel_scores = list(zip(micro_scores, eye_scores, voice_scores, physio_scores))\n        n_timeline = min(len(micro_scores), len(eye_scores), len(voice_scores))\n        for i in range(0, n_timeline, 5):\n            frame_channels = {\n                \"Micro-Expression\": micro_scores[i],\n                \"Eye Movement\":     eye_scores[i],\n                \"Voice Stress\":     voice_scores[i],\n                \"Physiological\":    physio_scores[min(i, len(physio_scores)-1)],\n                \"NLP Analysis\":     final_nlp,\n                \"Response Delay\":   final_delay,\n            }\n            self.fusion.fuse(frame_channels)\n\n        # â”€â”€â”€ Final channels â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n        final_channels = {\n            \"Micro-Expression\": final_micro,\n            \"Eye Movement\":     final_eye,\n            \"Voice Stress\":     final_voice,\n            \"NLP Analysis\":     final_nlp,\n            \"Response Delay\":   final_delay,\n            \"Physiological\":    final_physio,\n        }\n\n        duration = time.time() - start\n        result   = self.fusion.build_result(final_channels, raw_text=text, duration=duration)\n\n        print(f\"\\n{'â•'*55}\")\n        print(f\"  TRUTH SCORE  : {result.truth_score:.1f}%\")\n        print(f\"  VERDICT      : {result.verdict}\")\n        print(f\"  RISK LEVEL   : {result.deception_risk:.1f}%\")\n        print(f\"  FLAGS        : {len(result.flags)}\")\n        print(f\"  DURATION     : {duration:.2f}s\")\n        print(f\"{'â•'*55}\")\n\n        return result\n\n    def render_dashboard(self, result: AnalysisResult,\n                          save_path: str = None) -> plt.Figure:\n        \"\"\"Render and optionally save the analysis dashboard.\"\"\"\n        fig = self.dashboard.render(result)\n        if save_path:\n            fig.savefig(save_path, dpi=150, bbox_inches=\"tight\",\n                        facecolor=\"#030812\")\n            print(f\"  ğŸ’¾ Dashboard saved â†’ {save_path}\")\n        plt.show()\n        return fig\n\n\n# Instantiate the full pipeline\npipeline = VeritasPipeline()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n<a id=\"13-report\"></a>\n## ğŸ“„ Module 10 â€” Report Generator & Export"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# REPORT GENERATOR\n# ============================================================\n\nclass ReportGenerator:\n    \"\"\"\n    Generates professional text and JSON reports from AnalysisResult.\n    Supports:\n    - Plain text report (terminal-friendly)\n    - JSON export (machine-readable, full detail)\n    - Pandas DataFrame summary (for further analysis)\n    \"\"\"\n\n    @staticmethod\n    def text_report(result: AnalysisResult) -> str:\n        \"\"\"Generate formatted text report.\"\"\"\n        sep = \"â•\" * 62\n        thin = \"â”€\" * 62\n\n        lines = [\n            \"\",\n            sep,\n            \"  VERITAS â€” DECEPTION ANALYSIS REPORT\",\n            sep,\n            f\"  Session ID    : {result.session_id}\",\n            f\"  Timestamp     : {result.timestamp.strftime('%Y-%m-%d %H:%M:%S')}\",\n            f\"  Duration      : {result.duration_seconds:.2f} seconds\",\n            thin,\n            f\"  VERDICT       : {result.verdict}\",\n            f\"  TRUTH SCORE   : {result.truth_score:.1f}%\",\n            f\"  DECEPTION RISK: {result.deception_risk:.1f}%\",\n            thin,\n            \"  CHANNEL SCORES:\",\n        ]\n\n        for name, cs in result.channels.items():\n            bar_len = int(cs.truth_score / 5)\n            bar     = \"â–ˆ\" * bar_len + \"â–‘\" * (20 - bar_len)\n            lines.append(\n                f\"  {name:20s}  {bar}  {cs.truth_score:5.1f}%  [{cs.label}]\"\n            )\n\n        lines += [\n            thin,\n            f\"  DECEPTION FLAGS ({len(result.flags)}):\",\n        ]\n        if result.flags:\n            for flag in result.flags:\n                lines.append(f\"  âš   {flag}\")\n        else:\n            lines.append(\"  âœ…  No deception flags detected\")\n\n        if result.raw_text:\n            lines += [\n                thin,\n                \"  ANALYZED STATEMENT:\",\n                f\"  \"{result.raw_text[:200]}{'...' if len(result.raw_text) > 200 else ''}\"\",\n            ]\n\n        lines += [\n            thin,\n            \"  DISCLAIMER: Probabilistic output only. Not legal evidence.\",\n            sep,\n            \"\",\n        ]\n        return \"\\n\".join(lines)\n\n    @staticmethod\n    def to_json(result: AnalysisResult, filepath: str = None) -> str:\n        \"\"\"Export full analysis as JSON.\"\"\"\n        data = {\n            \"session_id\":      result.session_id,\n            \"timestamp\":       result.timestamp.isoformat(),\n            \"duration_seconds\":result.duration_seconds,\n            \"verdict\":         result.verdict,\n            \"truth_score\":     round(result.truth_score, 2),\n            \"deception_risk\":  round(result.deception_risk, 2),\n            \"flags\":           result.flags,\n            \"raw_text\":        result.raw_text,\n            \"channels\": {\n                name: {\n                    \"truth_score\":   round(cs.truth_score, 2),\n                    \"deception_risk\":round(cs.deception_risk, 2),\n                    \"confidence\":    round(cs.confidence, 3),\n                    \"label\":         cs.label,\n                    \"flags\":         cs.flags,\n                    \"metadata\":      cs.metadata,\n                }\n                for name, cs in result.channels.items()\n            },\n            \"timeline\":        [round(v, 2) for v in result.timeline],\n        }\n        json_str = json.dumps(data, indent=2)\n        if filepath:\n            with open(filepath, \"w\") as f:\n                f.write(json_str)\n            print(f\"  ğŸ’¾ JSON report saved â†’ {filepath}\")\n        return json_str\n\n    @staticmethod\n    def to_dataframe(result: AnalysisResult) -> pd.DataFrame:\n        \"\"\"Convert channel scores to Pandas DataFrame.\"\"\"\n        rows = []\n        for name, cs in result.channels.items():\n            rows.append({\n                \"Channel\":       name,\n                \"Truth Score\":   round(cs.truth_score, 1),\n                \"Deception Risk\":round(cs.deception_risk, 1),\n                \"Confidence\":    round(cs.confidence, 3),\n                \"Verdict\":       cs.label,\n                \"Flags\":         len(cs.flags),\n            })\n        df = pd.DataFrame(rows).set_index(\"Channel\")\n        return df\n\n    @staticmethod\n    def save_all(result: AnalysisResult, base_path: str = \"veritas_report\"):\n        \"\"\"Save text report, JSON, and CSV.\"\"\"\n        # Text\n        with open(f\"{base_path}.txt\", \"w\") as f:\n            f.write(ReportGenerator.text_report(result))\n        print(f\"  ğŸ’¾ Text report   â†’ {base_path}.txt\")\n        # JSON\n        ReportGenerator.to_json(result, f\"{base_path}.json\")\n        # CSV\n        df = ReportGenerator.to_dataframe(result)\n        df.to_csv(f\"{base_path}.csv\")\n        print(f\"  ğŸ’¾ CSV summary   â†’ {base_path}.csv\")\n        print(\"  âœ… All reports exported\")\n\n\nreporter = ReportGenerator()\nprint(\"âœ… ReportGenerator ready\")\nprint(\"   Outputs: .txt report | .json export | .csv dataframe\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n<a id=\"14-demo\"></a>\n## ğŸ§ª Section 14 â€” DEMO: Full Synthetic Simulation\n\nRun three complete analyses at different stress levels and compare results."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# DEMO â€” CASE 1: HIGH DECEPTION (Stress Level: 85%)\n# ============================================================\n\nprint(\"\\n\" + \"ğŸ”´ \" * 20)\nprint(\"CASE 1 â€” HIGH DECEPTION SUBJECT\")\nprint(\"ğŸ”´ \" * 20)\n\nresult_deceptive = pipeline.analyze_synthetic(\n    n_frames         = 100,\n    n_audio_segments = 50,\n    text = (\n        \"Trust me, I swear I wasn't anywhere near that location. \"\n        \"You have to believe me, I would never do something like that, honestly. \"\n        \"Like, I don't know, maybe someone else was there, I'm not sure, \"\n        \"sort of hard to remember exactly but I definitely wasn't involved.\"\n    ),\n    stress_level     = 0.85,\n    response_latency = 1.4,\n)\n\nprint(ReportGenerator.text_report(result_deceptive))\ndf1 = ReportGenerator.to_dataframe(result_deceptive)\ndisplay(df1.style.background_gradient(cmap=\"RdYlGn\", subset=[\"Truth Score\"],\n                                       vmin=0, vmax=100)\n              .format({\"Truth Score\": \"{:.1f}%\",\n                       \"Deception Risk\": \"{:.1f}%\",\n                       \"Confidence\": \"{:.0%}\"})\n              .set_caption(\"Case 1 â€” High Deception\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# DEMO â€” CASE 2: TRUTHFUL SUBJECT (Stress Level: 15%)\n# ============================================================\n\nprint(\"\\n\" + \"âœ… \" * 20)\nprint(\"CASE 2 â€” TRUTHFUL SUBJECT\")\nprint(\"âœ… \" * 20)\n\nresult_truthful = pipeline.analyze_synthetic(\n    n_frames         = 100,\n    n_audio_segments = 50,\n    text = (\n        \"I was at the downtown office at 3pm on Tuesday the 14th of March. \"\n        \"I had a 90-minute budget meeting with Sarah Chen and James Patel \"\n        \"from accounting. We reviewed the Q1 figures â€” revenue was up 12%. \"\n        \"I left at 4:35pm, drove home on Route 7, and arrived at 5:20pm.\"\n    ),\n    stress_level     = 0.15,\n    response_latency = 0.35,\n)\n\nprint(ReportGenerator.text_report(result_truthful))\ndf2 = ReportGenerator.to_dataframe(result_truthful)\ndisplay(df2.style.background_gradient(cmap=\"RdYlGn\", subset=[\"Truth Score\"],\n                                       vmin=0, vmax=100)\n              .format({\"Truth Score\": \"{:.1f}%\",\n                       \"Deception Risk\": \"{:.1f}%\",\n                       \"Confidence\": \"{:.0%}\"})\n              .set_caption(\"Case 2 â€” Truthful Subject\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# DEMO â€” CASE 3: INCONCLUSIVE / MIXED SIGNALS (Stress Level: 45%)\n# ============================================================\n\nprint(\"\\n\" + \"ğŸŸ¡ \" * 20)\nprint(\"CASE 3 â€” INCONCLUSIVE / MIXED SIGNALS\")\nprint(\"ğŸŸ¡ \" * 20)\n\nresult_mixed = pipeline.analyze_synthetic(\n    n_frames         = 100,\n    n_audio_segments = 50,\n    text = (\n        \"I think I was probably at home that evening, or maybe out. \"\n        \"I'm not entirely sure about the exact time, but I believe \"\n        \"it was sometime around 7 or 8pm. I usually stay in on weekdays.\"\n    ),\n    stress_level     = 0.45,\n    response_latency = 0.75,\n)\n\nprint(ReportGenerator.text_report(result_mixed))\ndf3 = ReportGenerator.to_dataframe(result_mixed)\ndisplay(df3.style.background_gradient(cmap=\"RdYlGn\", subset=[\"Truth Score\"],\n                                       vmin=0, vmax=100)\n              .format({\"Truth Score\": \"{:.1f}%\",\n                       \"Deception Risk\": \"{:.1f}%\",\n                       \"Confidence\": \"{:.0%}\"})\n              .set_caption(\"Case 3 â€” Inconclusive / Mixed\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# COMPARATIVE DASHBOARD â€” All 3 Cases\n# ============================================================\n\nfig, axes = plt.subplots(1, 3, figsize=(22, 7), facecolor=\"#030812\")\nfig.suptitle(\"VERITAS â€” COMPARATIVE ANALYSIS: 3 SUBJECTS\",\n             fontsize=14, fontweight=\"bold\", color=CYAN,\n             fontfamily=\"monospace\", y=1.01)\n\ncases = [\n    (\"SUBJECT A â€” DECEPTIVE\",  result_deceptive, RED),\n    (\"SUBJECT B â€” TRUTHFUL\",   result_truthful,  GREEN),\n    (\"SUBJECT C â€” INCONCLUSIVE\", result_mixed,   YELLOW),\n]\n\nfor ax, (title, result, c) in zip(axes, cases):\n    ax.set_facecolor(\"#080f1e\")\n    ax.set_title(title, color=c, fontsize=9, fontfamily=\"monospace\", pad=10)\n\n    names  = list(result.channels.keys())\n    scores = [result.channels[n].truth_score for n in names]\n    colors = [color_from_score(s) for s in scores]\n\n    y_pos = range(len(names))\n    ax.barh(y_pos, scores, color=colors, alpha=0.75, height=0.6, edgecolor=\"none\")\n    ax.set_yticks(list(y_pos))\n    ax.set_yticklabels([n.split()[0] for n in names], fontsize=8, fontfamily=\"monospace\")\n    ax.set_xlim(0, 105)\n    ax.set_xlabel(\"Truth Score %\", color=DIM, fontsize=8)\n    ax.tick_params(colors=DIM)\n    ax.axvline(50, color=YELLOW, linewidth=0.7, linestyle=\"--\", alpha=0.5)\n    ax.axvline(75, color=GREEN,  linewidth=0.7, linestyle=\"--\", alpha=0.5)\n    ax.grid(axis=\"x\", alpha=0.15)\n\n    for i, s in enumerate(scores):\n        ax.text(s + 1, i, f\"{s:.0f}%\", va=\"center\", fontsize=8,\n                color=colors[i], fontfamily=\"monospace\", fontweight=\"bold\")\n\n    # Overall score badge\n    ax.text(0.5, -0.18,\n            f\"OVERALL: {result.truth_score:.1f}%  â€”  {result.verdict}\",\n            transform=ax.transAxes, ha=\"center\", fontsize=8,\n            color=c, fontfamily=\"monospace\", fontweight=\"bold\",\n            bbox=dict(boxstyle=\"round\", facecolor=\"#080f1e\", edgecolor=c, alpha=0.8))\n\nplt.tight_layout()\nplt.savefig(\"veritas_comparison.png\", dpi=150, bbox_inches=\"tight\",\n            facecolor=\"#030812\")\nplt.show()\nprint(\"ğŸ’¾ Comparison chart saved â†’ veritas_comparison.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# RENDER FULL DASHBOARD â€” Most Interesting Case\n# ============================================================\n\nprint(\"\\nğŸ“Š Rendering full VERITAS dashboard for deceptive subject...\")\nfig = pipeline.render_dashboard(result_deceptive, save_path=\"veritas_dashboard.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# EXPORT ALL REPORTS\n# ============================================================\n\nprint(\"ğŸ“„ Exporting reports for all 3 cases...\\n\")\n\nReportGenerator.save_all(result_deceptive, \"report_deceptive\")\nprint()\nReportGenerator.save_all(result_truthful,  \"report_truthful\")\nprint()\nReportGenerator.save_all(result_mixed,     \"report_mixed\")\n\nprint(\"\\nâœ… All reports exported successfully!\")\nprint(\"   Files: report_*.txt | report_*.json | report_*.csv\")\nprint(\"   Images: veritas_dashboard.png | veritas_comparison.png\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# ============================================================\n# QUICK TEXT ANALYSIS â€” Analyze any statement\n# ============================================================\n\ndef quick_analyze(statement: str, latency: float = 0.5):\n    \"\"\"\n    Quick NLP analysis of any text statement.\n    Usage: quick_analyze(\"your statement here\")\n    \"\"\"\n    result = pipeline.analyze_text(statement, response_latency=latency)\n    print(ReportGenerator.text_report(result))\n    return result\n\n# â”€â”€ Try your own statement â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nmy_statement = \"I was home all night. I didn't go anywhere.\"\n\nresult = quick_analyze(my_statement, latency=0.4)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n\n<div align=\"center\">\n\n```\nVERITAS v2.4  Â·  Python Notebook Edition\nMicro-Expressions Â· Eye Movement Â· Voice Stress Â· NLP Â· Delay Â· Physiological\nZero external API calls Â· All processing local\n```\n\n</div>"
   ]
  }
 ]
}