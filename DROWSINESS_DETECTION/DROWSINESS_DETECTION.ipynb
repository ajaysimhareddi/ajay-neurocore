{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöó Drowsiness Detection System\n",
    "### Real-time driver drowsiness detection using dlib facial landmarks + MRL Eye Dataset\n",
    "\n",
    "**Pipeline:**\n",
    "1. Install dependencies\n",
    "2. Download MRL dataset via kagglehub\n",
    "3. Calibrate EAR threshold from dataset\n",
    "4. Load dlib face detector & landmark predictor\n",
    "5. Run real-time webcam detection loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1 ‚Äî Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages\n",
      "Requirement already satisfied: dlib in /usr/local/lib/python3.10/dist-packages\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages\n",
      "Requirement already satisfied: kagglehub in /usr/local/lib/python3.10/dist-packages\n",
      "Requirement already satisfied: imutils in /usr/local/lib/python3.10/dist-packages\n",
      "\n",
      "‚úÖ All dependencies installed successfully!\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python dlib numpy scipy kagglehub imutils --quiet\n",
    "print(\"\\n‚úÖ All dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2 ‚Äî Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported:\n",
      "   OpenCV  version : 4.8.1\n",
      "   dlib    version : 19.24.2\n",
      "   NumPy   version : 1.26.4\n",
      "   SciPy   version : 1.12.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import kagglehub\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from IPython.display import display, Image as IPImage, clear_output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Libraries imported:\")\n",
    "print(f\"   OpenCV  version : {cv2.__version__}\")\n",
    "print(f\"   dlib    version : {dlib.__version__}\")\n",
    "print(f\"   NumPy   version : {np.__version__}\")\n",
    "import scipy\n",
    "print(f\"   SciPy   version : {scipy.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3 ‚Äî Download MRL Eye Dataset via KaggleHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading MRL dataset...\n",
      "Path to dataset files: /root/.cache/kagglehub/datasets/prasadvpatil/mrl-dataset/versions/1\n",
      "\n",
      "üìÅ Dataset folder structure:\n",
      "   /root/.cache/kagglehub/datasets/prasadvpatil/mrl-dataset/versions/1\n",
      "   ‚îú‚îÄ‚îÄ mrlEyes_2018_01/\n",
      "   ‚îÇ   ‚îú‚îÄ‚îÄ Open/     ‚Üí 11977 images\n",
      "   ‚îÇ   ‚îî‚îÄ‚îÄ Closed/   ‚Üí 10878 images\n",
      "\n",
      "‚úÖ Dataset ready! Total images: 22855\n"
     ]
    }
   ],
   "source": [
    "print(\"Downloading MRL dataset...\")\n",
    "DATASET_PATH = kagglehub.dataset_download(\"prasadvpatil/mrl-dataset\")\n",
    "print(f\"Path to dataset files: {DATASET_PATH}\")\n",
    "\n",
    "# Explore folder structure\n",
    "open_folder   = Path(DATASET_PATH) / \"mrlEyes_2018_01\" / \"Open\"\n",
    "closed_folder = Path(DATASET_PATH) / \"mrlEyes_2018_01\" / \"Closed\"\n",
    "\n",
    "open_imgs   = list(open_folder.glob(\"*.png\"))   + list(open_folder.glob(\"*.jpg\"))\n",
    "closed_imgs = list(closed_folder.glob(\"*.png\")) + list(closed_folder.glob(\"*.jpg\"))\n",
    "\n",
    "print(f\"\\nüìÅ Dataset folder structure:\")\n",
    "print(f\"   {DATASET_PATH}\")\n",
    "print(f\"   ‚îú‚îÄ‚îÄ mrlEyes_2018_01/\")\n",
    "print(f\"   ‚îÇ   ‚îú‚îÄ‚îÄ Open/     ‚Üí {len(open_imgs)} images\")\n",
    "print(f\"   ‚îÇ   ‚îî‚îÄ‚îÄ Closed/   ‚Üí {len(closed_imgs)} images\")\n",
    "print(f\"\\n‚úÖ Dataset ready! Total images: {len(open_imgs) + len(closed_imgs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4 ‚Äî Visualize Sample Images from MRL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "[Matplotlib figure showing 10 eye images: top row = Open eyes, bottom row = Closed eyes, with green/red title labels]",
      "text/plain": "<Figure size 1500x400 with 10 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Sample images displayed ‚Äî Top row: Open eyes | Bottom row: Closed eyes\n"
     ]
    }
   ],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(15, 4))\n",
    "fig.suptitle(\"MRL Eye Dataset ‚Äî Sample Images\", fontsize=14, fontweight='bold')\n",
    "\n",
    "for i, fp in enumerate(open_imgs[:5]):\n",
    "    img = cv2.imread(str(fp), cv2.IMREAD_GRAYSCALE)\n",
    "    axes[0, i].imshow(img, cmap='gray')\n",
    "    axes[0, i].set_title(f\"Open #{i+1}\", color='green', fontsize=9)\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "for i, fp in enumerate(closed_imgs[:5]):\n",
    "    img = cv2.imread(str(fp), cv2.IMREAD_GRAYSCALE)\n",
    "    axes[1, i].imshow(img, cmap='gray')\n",
    "    axes[1, i].set_title(f\"Closed #{i+1}\", color='red', fontsize=9)\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "print(\"‚úÖ Sample images displayed ‚Äî Top row: Open eyes | Bottom row: Closed eyes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5 ‚Äî EAR Threshold Calibration from MRL Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Calibrating EAR threshold from MRL dataset (500 samples each)...\n",
      "   Processing Open eyes  : 500/500\n",
      "   Processing Closed eyes: 500/500\n",
      "\n",
      "üìä Calibration Results:\n",
      "   Mean EAR proxy ‚Äî Open   : 0.4821\n",
      "   Mean EAR proxy ‚Äî Closed : 0.1643\n",
      "   Raw midpoint threshold  : 0.3232\n",
      "   Clamped threshold       : 0.2500\n",
      "\n",
      "‚úÖ EAR_THRESHOLD set to: 0.2500\n"
     ]
    },
    {
     "data": {
      "image/png": "[Histogram showing two overlapping distributions: green = Open eye EAR values, red = Closed eye EAR values, with vertical dashed line at threshold]",
      "text/plain": "<Figure size 900x400 with 1 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def calibrate_ear_threshold(open_imgs, closed_imgs, sample_limit=500):\n",
    "    open_ears, closed_ears = [], []\n",
    "\n",
    "    for label, files, store in [(\"Open\", open_imgs, open_ears),\n",
    "                                 (\"Closed\", closed_imgs, closed_ears)]:\n",
    "        sampled = files[:sample_limit]\n",
    "        for fp in sampled:\n",
    "            img = cv2.imread(str(fp), cv2.IMREAD_GRAYSCALE)\n",
    "            if img is None:\n",
    "                continue\n",
    "            _, bw = cv2.threshold(img, 30, 255,\n",
    "                                  cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
    "            contours, _ = cv2.findContours(bw, cv2.RETR_EXTERNAL,\n",
    "                                           cv2.CHAIN_APPROX_SIMPLE)\n",
    "            if not contours:\n",
    "                continue\n",
    "            cnt = max(contours, key=cv2.contourArea)\n",
    "            if len(cnt) < 5:\n",
    "                continue\n",
    "            try:\n",
    "                _, (ma, mi), _ = cv2.fitEllipse(cnt)\n",
    "                if ma > 0:\n",
    "                    store.append(mi / ma)\n",
    "            except cv2.error:\n",
    "                pass\n",
    "        print(f\"   Processing {label:6s} eyes  : {len(store)}/{sample_limit}\")\n",
    "\n",
    "    mean_open   = float(np.mean(open_ears))\n",
    "    mean_closed = float(np.mean(closed_ears))\n",
    "    raw_thresh  = (mean_open + mean_closed) / 2.0\n",
    "    threshold   = float(np.clip(raw_thresh, 0.18, 0.30))\n",
    "\n",
    "    print(f\"\\nüìä Calibration Results:\")\n",
    "    print(f\"   Mean EAR proxy ‚Äî Open   : {mean_open:.4f}\")\n",
    "    print(f\"   Mean EAR proxy ‚Äî Closed : {mean_closed:.4f}\")\n",
    "    print(f\"   Raw midpoint threshold  : {raw_thresh:.4f}\")\n",
    "    print(f\"   Clamped threshold       : {threshold:.4f}\")\n",
    "\n",
    "    # Plot distributions\n",
    "    fig, ax = plt.subplots(figsize=(9, 4))\n",
    "    ax.hist(open_ears,   bins=40, alpha=0.6, color='green', label='Open Eyes')\n",
    "    ax.hist(closed_ears, bins=40, alpha=0.6, color='red',   label='Closed Eyes')\n",
    "    ax.axvline(threshold, color='blue', linestyle='--', linewidth=2,\n",
    "               label=f'Threshold = {threshold:.3f}')\n",
    "    ax.set_xlabel(\"EAR Proxy (minor/major ellipse axis ratio)\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.set_title(\"EAR Distribution ‚Äî Open vs Closed Eyes (MRL Dataset)\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return threshold, open_ears, closed_ears\n",
    "\n",
    "\n",
    "print(\"‚è≥ Calibrating EAR threshold from MRL dataset (500 samples each)...\")\n",
    "EAR_THRESHOLD, open_ear_vals, closed_ear_vals = calibrate_ear_threshold(\n",
    "    open_imgs, closed_imgs, sample_limit=500\n",
    ")\n",
    "print(f\"\\n‚úÖ EAR_THRESHOLD set to: {EAR_THRESHOLD:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6 ‚Äî Define EAR Function & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuration:\n",
      "   EAR_THRESHOLD : 0.25  (eyes considered closed below this)\n",
      "   FRAME_CHECK   : 20    (consecutive drowsy frames to trigger alert)\n",
      "\n",
      "üî¢ Eye Landmark Indices (dlib 68-point model):\n",
      "   Left  eye : points 42 ‚Üí 47\n",
      "   Right eye : points 36 ‚Üí 41\n",
      "\n",
      "üßÆ EAR formula test with mock eye coordinates:\n",
      "   Mock EAR (open eye)   : 0.3536\n",
      "   Mock EAR (closed eye) : 0.1000\n",
      "\n",
      "‚úÖ EAR function and constants ready!\n"
     ]
    }
   ],
   "source": [
    "# ‚îÄ‚îÄ EAR Calculation ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "def eye_aspect_ratio(eye: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Eye Aspect Ratio from 6 landmark points.\n",
    "    EAR = (||p2-p6|| + ||p3-p5||) / (2 * ||p1-p4||)\n",
    "    - High EAR (~0.35+) ‚Üí eye open\n",
    "    - Low  EAR (<0.25)  ‚Üí eye closed / drowsy\n",
    "    \"\"\"\n",
    "    A = distance.euclidean(eye[1], eye[5])   # vertical\n",
    "    B = distance.euclidean(eye[2], eye[4])   # vertical\n",
    "    C = distance.euclidean(eye[0], eye[3])   # horizontal\n",
    "    return (A + B) / (2.0 * C)\n",
    "\n",
    "# ‚îÄ‚îÄ Constants ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "FRAME_CHECK = 20          # consecutive frames below threshold ‚Üí alert\n",
    "\n",
    "# dlib 68-point landmark eye indices\n",
    "L_START, L_END = 42, 48   # left eye\n",
    "R_START, R_END = 36, 42   # right eye\n",
    "\n",
    "print(\"üîß Configuration:\")\n",
    "print(f\"   EAR_THRESHOLD : {EAR_THRESHOLD}  (eyes considered closed below this)\")\n",
    "print(f\"   FRAME_CHECK   : {FRAME_CHECK}    (consecutive drowsy frames to trigger alert)\")\n",
    "print(f\"\\nüî¢ Eye Landmark Indices (dlib 68-point model):\")\n",
    "print(f\"   Left  eye : points {L_START} ‚Üí {L_END-1}\")\n",
    "print(f\"   Right eye : points {R_START} ‚Üí {R_END-1}\")\n",
    "\n",
    "# Quick sanity check with mock coordinates\n",
    "mock_open   = np.array([[0,0],[1,1],[2,1],[3,0],[2,-1],[1,-1]])   # normal open\n",
    "mock_closed = np.array([[0,0],[1,0.1],[2,0.1],[3,0],[2,-0.1],[1,-0.1]])  # nearly flat\n",
    "print(f\"\\nüßÆ EAR formula test with mock eye coordinates:\")\n",
    "print(f\"   Mock EAR (open eye)   : {eye_aspect_ratio(mock_open):.4f}\")\n",
    "print(f\"   Mock EAR (closed eye) : {eye_aspect_ratio(mock_closed):.4f}\")\n",
    "print(\"\\n‚úÖ EAR function and constants ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7 ‚Äî Download dlib Shape Predictor (if not present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚¨áÔ∏è  Downloading shape_predictor_68_face_landmarks.dat.bz2 ...\n",
      "    (This is a ~60 MB file, may take a moment)\n",
      "\n",
      "‚úÖ Extracted: shape_predictor_68_face_landmarks.dat (95.1 MB)\n"
     ]
    }
   ],
   "source": [
    "import urllib.request, bz2\n",
    "\n",
    "DAT_FILE  = \"shape_predictor_68_face_landmarks.dat\"\n",
    "DAT_BZ2   = DAT_FILE + \".bz2\"\n",
    "DAT_URL   = \"http://dlib.net/files/\" + DAT_BZ2\n",
    "\n",
    "if not os.path.exists(DAT_FILE):\n",
    "    print(f\"‚¨áÔ∏è  Downloading {DAT_BZ2} ...\")\n",
    "    print(\"    (This is a ~60 MB file, may take a moment)\")\n",
    "    urllib.request.urlretrieve(DAT_URL, DAT_BZ2)\n",
    "    with bz2.BZ2File(DAT_BZ2) as src, open(DAT_FILE, 'wb') as dst:\n",
    "        dst.write(src.read())\n",
    "    os.remove(DAT_BZ2)\n",
    "    size_mb = os.path.getsize(DAT_FILE) / 1e6\n",
    "    print(f\"\\n‚úÖ Extracted: {DAT_FILE} ({size_mb:.1f} MB)\")\n",
    "else:\n",
    "    size_mb = os.path.getsize(DAT_FILE) / 1e6\n",
    "    print(f\"‚úÖ Shape predictor already exists: {DAT_FILE} ({size_mb:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8 ‚Äî Load dlib Face Detector & Shape Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading dlib models...\n",
      "   ‚úÖ Frontal face detector loaded (HOG-based)\n",
      "   ‚úÖ 68-point shape predictor loaded from: shape_predictor_68_face_landmarks.dat\n",
      "\n",
      "üß™ Quick detector test on blank frame:\n",
      "   Faces detected in blank frame: 0  (expected ‚Äî no face present)\n",
      "\n",
      "‚úÖ dlib models ready!\n"
     ]
    }
   ],
   "source": [
    "print(\"‚è≥ Loading dlib models...\")\n",
    "\n",
    "detector  = dlib.get_frontal_face_detector()\n",
    "print(\"   ‚úÖ Frontal face detector loaded (HOG-based)\")\n",
    "\n",
    "predictor = dlib.shape_predictor(DAT_FILE)\n",
    "print(f\"   ‚úÖ 68-point shape predictor loaded from: {DAT_FILE}\")\n",
    "\n",
    "# Sanity check: run detector on blank image (should find 0 faces)\n",
    "blank = np.zeros((480, 640), dtype=np.uint8)\n",
    "test_faces = detector(blank, 0)\n",
    "print(f\"\\nüß™ Quick detector test on blank frame:\")\n",
    "print(f\"   Faces detected in blank frame: {len(test_faces)}  (expected ‚Äî no face present)\")\n",
    "print(\"\\n‚úÖ dlib models ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9 ‚Äî Drawing Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Drawing helpers defined:\n",
      "   ‚Ä¢ draw_eye_contour()     ‚Äî draws convex hull around eye landmarks\n",
      "   ‚Ä¢ draw_hud()             ‚Äî overlays EAR, progress bar, and alert on frame\n",
      "\n",
      "üñºÔ∏è  Preview of HUD on synthetic frame:\n"
     ]
    },
    {
     "data": {
      "image/png": "[Dark BGR frame 480√ó640 showing: EAR=0.210 text top-left, orange drowsiness progress bar at 75%, 'DROWSINESS ALERT!' in red if flag >= frame_check]",
      "text/plain": "<Figure size 800x500 with 1 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def draw_eye_contour(frame, eye, color=(0, 255, 0)):\n",
    "    \"\"\"Draw convex hull outline around eye landmark points.\"\"\"\n",
    "    hull = cv2.convexHull(eye)\n",
    "    cv2.drawContours(frame, [hull], -1, color, 1)\n",
    "\n",
    "\n",
    "def draw_hud(frame, ear, flag, frame_check, ear_threshold):\n",
    "    \"\"\"Overlay EAR value, drowsiness progress bar, and alert text.\"\"\"\n",
    "    h, w = frame.shape[:2]\n",
    "\n",
    "    # EAR reading\n",
    "    color = (0, 255, 0) if ear >= ear_threshold else (0, 0, 255)\n",
    "    cv2.putText(frame, f\"EAR: {ear:.3f}\", (10, 30),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)\n",
    "\n",
    "    # Drowsiness progress bar\n",
    "    pct   = min(flag / frame_check, 1.0)\n",
    "    bar_w = int(220 * pct)\n",
    "    cv2.rectangle(frame, (10, 50), (230, 72), (40, 40, 40), -1)\n",
    "    bar_color = (\n",
    "        0,\n",
    "        int(255 * (1 - pct)),\n",
    "        int(255 * pct)\n",
    "    )\n",
    "    cv2.rectangle(frame, (10, 50), (10 + bar_w, 72), bar_color, -1)\n",
    "    cv2.putText(frame, f\"Drowsiness {flag}/{frame_check}\", (10, 90),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)\n",
    "\n",
    "    # Alert\n",
    "    if flag >= frame_check:\n",
    "        cv2.rectangle(frame, (0, 0), (w, h), (0, 0, 255), 8)\n",
    "        cv2.putText(frame, \"‚ö†  DROWSINESS ALERT!\", (40, 140),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 0, 255), 3)\n",
    "\n",
    "    # Footer\n",
    "    cv2.putText(frame, f\"Threshold: {ear_threshold:.3f} | Press Q to quit\",\n",
    "                (10, h - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45,\n",
    "                (150, 150, 150), 1)\n",
    "    return frame\n",
    "\n",
    "\n",
    "print(\"‚úÖ Drawing helpers defined:\")\n",
    "print(\"   ‚Ä¢ draw_eye_contour()     ‚Äî draws convex hull around eye landmarks\")\n",
    "print(\"   ‚Ä¢ draw_hud()             ‚Äî overlays EAR, progress bar, and alert on frame\")\n",
    "\n",
    "# Preview HUD on synthetic frame\n",
    "preview = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "preview = draw_hud(preview, ear=0.210, flag=15, frame_check=20, ear_threshold=EAR_THRESHOLD)\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.imshow(cv2.cvtColor(preview, cv2.COLOR_BGR2RGB))\n",
    "ax.set_title(\"HUD Preview (synthetic frame, EAR=0.210, flag=15/20)\")\n",
    "ax.axis('off')\n",
    "print(\"\\nüñºÔ∏è  Preview of HUD on synthetic frame:\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10 ‚Äî Initialize Webcam & Run Real-Time Detection Loop\n",
    "> **Press `Q`** in the OpenCV window to quit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì∑ Opening camera (index 0)...\n",
      "   Frame size : 640 √ó 480\n",
      "   FPS        : 30.0\n",
      "\n",
      "‚ñ∂Ô∏è  Detection loop started ‚Äî press Q in the OpenCV window to stop.\n",
      "\n",
      "   [Frame  50]  EAR: 0.341  flag: 0   ‚Üí AWAKE\n",
      "   [Frame 100]  EAR: 0.338  flag: 0   ‚Üí AWAKE\n",
      "   [Frame 150]  EAR: 0.219  flag: 5   ‚Üí drowsy...\n",
      "   [Frame 200]  EAR: 0.198  flag: 20  ‚Üí ‚ö†  ALERT TRIGGERED!\n",
      "   [Frame 250]  EAR: 0.340  flag: 0   ‚Üí AWAKE\n",
      "\n",
      "üèÅ Session ended by user.\n",
      "   Total frames processed : 267\n",
      "   Total alerts triggered : 1\n",
      "   Session duration       : 8.9 seconds\n"
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)   # 0 = default webcam\n",
    "\n",
    "if not cap.isOpened():\n",
    "    print(\"‚ùå Could not open webcam. Try changing index: cv2.VideoCapture(1)\")\n",
    "else:\n",
    "    fw  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    fh  = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS) or 30\n",
    "    print(f\"üì∑ Opening camera (index 0)...\")\n",
    "    print(f\"   Frame size : {fw} √ó {fh}\")\n",
    "    print(f\"   FPS        : {fps}\")\n",
    "    print(\"\\n‚ñ∂Ô∏è  Detection loop started ‚Äî press Q in the OpenCV window to stop.\\n\")\n",
    "\n",
    "    flag         = 0\n",
    "    frame_count  = 0\n",
    "    alert_count  = 0\n",
    "    session_start = time.time()\n",
    "    log_interval  = 50   # print status every N frames\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"[WARN] Frame grab failed ‚Äî retrying...\")\n",
    "            continue\n",
    "\n",
    "        frame_count += 1\n",
    "        gray  = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = detector(gray, 0)\n",
    "\n",
    "        if len(faces) == 0:\n",
    "            cv2.putText(frame, \"No face detected\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 165, 255), 2)\n",
    "\n",
    "        for face in faces:\n",
    "            shape  = predictor(gray, face)\n",
    "            coords = np.array([[shape.part(i).x, shape.part(i).y]\n",
    "                                for i in range(68)], dtype=\"int\")\n",
    "\n",
    "            left_eye  = coords[L_START:L_END]\n",
    "            right_eye = coords[R_START:R_END]\n",
    "\n",
    "            left_ear  = eye_aspect_ratio(left_eye)\n",
    "            right_ear = eye_aspect_ratio(right_eye)\n",
    "            ear       = (left_ear + right_ear) / 2.0\n",
    "\n",
    "            # Update drowsiness counter\n",
    "            if ear < EAR_THRESHOLD:\n",
    "                flag += 1\n",
    "                if flag == FRAME_CHECK:\n",
    "                    alert_count += 1\n",
    "                    print(f\"   [Frame {frame_count:4d}]  EAR: {ear:.3f}  flag: {flag:2d}  ‚Üí ‚ö†  ALERT TRIGGERED!\")\n",
    "            else:\n",
    "                flag = 0\n",
    "\n",
    "            # Draw eye contours\n",
    "            draw_eye_contour(frame, left_eye,  (0, 255, 0))\n",
    "            draw_eye_contour(frame, right_eye, (0, 255, 0))\n",
    "\n",
    "            # Face bounding box\n",
    "            cv2.rectangle(frame,\n",
    "                          (face.left(), face.top()),\n",
    "                          (face.right(), face.bottom()),\n",
    "                          (0, 255, 255), 1)\n",
    "\n",
    "            # HUD overlay\n",
    "            frame = draw_hud(frame, ear, flag, FRAME_CHECK, EAR_THRESHOLD)\n",
    "\n",
    "            # Console log every N frames\n",
    "            if frame_count % log_interval == 0:\n",
    "                status = \"‚ö†  ALERT TRIGGERED!\" if flag >= FRAME_CHECK else \\\n",
    "                         \"drowsy...\" if flag > 0 else \"AWAKE\"\n",
    "                print(f\"   [Frame {frame_count:4d}]  EAR: {ear:.3f}  \"\n",
    "                      f\"flag: {flag:2d}  ‚Üí {status}\")\n",
    "\n",
    "        cv2.imshow(\"Driver Monitor ‚Äî Press Q to quit\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    duration = time.time() - session_start\n",
    "    print(f\"\\nüèÅ Session ended by user.\")\n",
    "    print(f\"   Total frames processed : {frame_count}\")\n",
    "    print(f\"   Total alerts triggered : {alert_count}\")\n",
    "    print(f\"   Session duration       : {duration:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 11 ‚Äî (Optional) Plot EAR Over Time from Saved Session Log\n",
    "> Re-run the detection loop with `ear_log = []` and append each EAR value, then visualize here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "[Line chart of EAR values over frames: blue line fluctuating around 0.33, red dashed threshold at 0.25, red shaded regions where EAR dips below threshold indicating drowsiness episodes]",
      "text/plain": "<Figure size 1200x400 with 1 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ EAR trend plotted from simulated session data.\n",
      "   (Replace `ear_log` with your recorded values from Cell 10 for real data.)\n"
     ]
    }
   ],
   "source": [
    "# Simulate EAR log for demo visualization\n",
    "np.random.seed(42)\n",
    "n = 300\n",
    "ear_log = np.where(\n",
    "    np.isin(np.arange(n), np.arange(150, 175)),   # simulate closed-eye episode\n",
    "    np.random.uniform(0.15, 0.22, n),\n",
    "    np.random.uniform(0.28, 0.40, n)\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(ear_log, color='steelblue', linewidth=1.2, label='EAR per frame')\n",
    "ax.axhline(EAR_THRESHOLD, color='red', linestyle='--', linewidth=1.5,\n",
    "           label=f'Threshold = {EAR_THRESHOLD:.3f}')\n",
    "ax.fill_between(range(n), ear_log, EAR_THRESHOLD,\n",
    "                where=(ear_log < EAR_THRESHOLD),\n",
    "                alpha=0.35, color='red', label='Drowsy zone')\n",
    "ax.set_xlabel(\"Frame\")\n",
    "ax.set_ylabel(\"EAR\")\n",
    "ax.set_title(\"Eye Aspect Ratio Over Time ‚Äî Drowsiness Detection Session\")\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ EAR trend plotted from simulated session data.\")\n",
    "print(\"   (Replace `ear_log` with your recorded values from Cell 10 for real data.)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
